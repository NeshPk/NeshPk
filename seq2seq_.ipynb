{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NeshPk/NeshPk/blob/main/seq2seq_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "42jC03SnLF8d",
        "outputId": "d5af1b24-1fcd-4a8a-ec63-7154ae6bd2e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using data path: fra.txt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'fra.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-1254690894.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0minput_characters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0mtarget_characters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'fra.txt'"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Title: Character-level recurrent sequence-to-sequence model\n",
        "Author: [fchollet](https://twitter.com/fchollet)\n",
        "Date created: 2017/09/29\n",
        "Last modified: 2023/11/22\n",
        "Description: Character-level recurrent sequence-to-sequence model.\n",
        "Accelerator: GPU\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "## Introduction\n",
        "\n",
        "This example demonstrates how to implement a basic character-level\n",
        "recurrent sequence-to-sequence model. We apply it to translating\n",
        "short English sentences into short French sentences,\n",
        "character-by-character. Note that it is fairly unusual to\n",
        "do character-level machine translation, as word-level\n",
        "models are more common in this domain.\n",
        "\n",
        "**Summary of the algorithm**\n",
        "\n",
        "- We start with input sequences from a domain (e.g. English sentences)\n",
        "    and corresponding target sequences from another domain\n",
        "    (e.g. French sentences).\n",
        "- An encoder LSTM turns input sequences to 2 state vectors\n",
        "    (we keep the last LSTM state and discard the outputs).\n",
        "- A decoder LSTM is trained to turn the target sequences into\n",
        "    the same sequence but offset by one timestep in the future,\n",
        "    a training process called \"teacher forcing\" in this context.\n",
        "    It uses as initial state the state vectors from the encoder.\n",
        "    Effectively, the decoder learns to generate `targets[t+1...]`\n",
        "    given `targets[...t]`, conditioned on the input sequence.\n",
        "- In inference mode, when we want to decode unknown input sequences, we:\n",
        "    - Encode the input sequence into state vectors\n",
        "    - Start with a target sequence of size 1\n",
        "        (just the start-of-sequence character)\n",
        "    - Feed the state vectors and 1-char target sequence\n",
        "        to the decoder to produce predictions for the next character\n",
        "    - Sample the next character using these predictions\n",
        "        (we simply use argmax).\n",
        "    - Append the sampled character to the target sequence\n",
        "    - Repeat until we generate the end-of-sequence character or we\n",
        "        hit the character limit.\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "## Setup\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import keras\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "\"\"\"\n",
        "## Download the data\n",
        "\"\"\"\n",
        "\n",
        "#fpath = keras.utils.get_file(origin=\"http://www.manythings.org/anki/fra-eng.zip\")\n",
        "#dirpath = Path(fpath).parent.absolute()\n",
        "#os.system(f\"unzip -q {fpath} -d {dirpath}\")\n",
        "\n",
        "data_path = 'fra.txt' # Assumes it's in the main /content/ directory\n",
        "print(f\"Using data path: {data_path}\")\n",
        "\n",
        "\"\"\"\n",
        "## Configuration\n",
        "\"\"\"\n",
        "\n",
        "batch_size = 64  # Batch size for training.\n",
        "epochs = 100  # Number of epochs to train for.\n",
        "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
        "num_samples = 10000  # Number of samples to train on.\n",
        "# Path to the data txt file on disk.\n",
        "#data_path = os.path.join(dirpath, \"fra.txt\")\n",
        "\n",
        "\"\"\"\n",
        "## Prepare the data\n",
        "\"\"\"\n",
        "\n",
        "# Vectorize the data.\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.read().split(\"\\n\")\n",
        "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "    input_text, target_text, _ = line.split(\"\\t\")\n",
        "    # We use \"tab\" as the \"start sequence\" character\n",
        "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "    target_text = \"\\t\" + target_text + \"\\n\"\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "    for char in input_text:\n",
        "        if char not in input_characters:\n",
        "            input_characters.add(char)\n",
        "    for char in target_text:\n",
        "        if char not in target_characters:\n",
        "            target_characters.add(char)\n",
        "\n",
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "print(\"Number of samples:\", len(input_texts))\n",
        "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
        "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
        "\n",
        "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
        "\n",
        "encoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
        "    dtype=\"float32\",\n",
        ")\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype=\"float32\",\n",
        ")\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype=\"float32\",\n",
        ")\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
        "    encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n",
        "    for t, char in enumerate(target_text):\n",
        "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "        decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep\n",
        "            # and will not include the start character.\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "    decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n",
        "    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0\n",
        "\n",
        "\"\"\"\n",
        "## Build the model\n",
        "\"\"\"\n",
        "\n",
        "# Define an input sequence and process it.\n",
        "encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\n",
        "encoder = keras.layers.LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\n",
        "\n",
        "# We set up our decoder to return full output sequences,\n",
        "# and to return internal states as well. We don't use the\n",
        "# return states in the training model, but we will use them in inference.\n",
        "decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "\"\"\"\n",
        "## Train the model\n",
        "\"\"\"\n",
        "\n",
        "model.compile(\n",
        "    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "model.fit(\n",
        "    [encoder_input_data, decoder_input_data],\n",
        "    decoder_target_data,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_split=0.2,\n",
        ")\n",
        "# Save model\n",
        "model.save(\"s2s_model.keras\")\n",
        "\n",
        "\"\"\"\n",
        "## Run inference (sampling)\n",
        "\n",
        "1. encode input and retrieve initial decoder state\n",
        "2. run one step of decoder with this initial state\n",
        "and a \"start of sequence\" token as target.\n",
        "Output will be the next target token.\n",
        "3. Repeat with the current target token and current states\n",
        "\"\"\"\n",
        "\n",
        "# Define sampling models\n",
        "# Restore the model and construct the encoder and decoder.\n",
        "model = keras.models.load_model(\"s2s_model.keras\")\n",
        "\n",
        "encoder_inputs = model.input[0]  # input_1\n",
        "encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1\n",
        "encoder_states = [state_h_enc, state_c_enc]\n",
        "encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_inputs = model.input[1]  # input_2\n",
        "decoder_state_input_h = keras.Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = keras.Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_lstm = model.layers[3]\n",
        "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
        "    decoder_inputs, initial_state=decoder_states_inputs\n",
        ")\n",
        "decoder_states = [state_h_dec, state_c_dec]\n",
        "decoder_dense = model.layers[4]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = keras.Model(\n",
        "    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
        ")\n",
        "\n",
        "# Reverse-lookup token index to decode sequences back to\n",
        "# something readable.\n",
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
        "\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq, verbose=0)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = \"\"\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq] + states_value, verbose=0\n",
        "        )\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.0\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "    return decoded_sentence\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "You can now generate decoded sentences as such:\n",
        "\"\"\"\n",
        "\n",
        "for seq_index in range(20):\n",
        "    # Take one sequence (part of the training set)\n",
        "    # for trying out decoding.\n",
        "    input_seq = encoder_input_data[seq_index : seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    print(\"-\")\n",
        "    print(\"Input sentence:\", input_texts[seq_index])\n",
        "    print(\"Decoded sentence:\", decoded_sentence)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/yunjey/pytorch-tutorial.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXm5U1iiLwNH",
        "outputId": "85eaf6d7-ec96-4f69-b7fe-d608d8614117"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pytorch-tutorial'...\n",
            "remote: Enumerating objects: 917, done.\u001b[K\n",
            "remote: Total 917 (delta 0), reused 0 (delta 0), pack-reused 917 (from 1)\u001b[K\n",
            "Receiving objects: 100% (917/917), 12.80 MiB | 7.12 MiB/s, done.\n",
            "Resolving deltas: 100% (491/491), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd pytorch-tutorial/tutorials/03-advanced/image_captioning/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFZxIvFpRgOp",
        "outputId": "5e0f9c21-c22b-43ae-cbd0-1d1152f91882"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pytorch-tutorial/tutorials/03-advanced/image_captioning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "w7YhOqnDRqEQ",
        "outputId": "e1f74554-cca5-4805-e417-c40f725e7dd9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (3.10.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (2.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (11.2.1)\n",
            "Collecting argparse (from -r requirements.txt (line 5))\n",
            "  Using cached argparse-1.4.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 1)) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 1)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 1)) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 1)) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 1)) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 1)) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 1)) (2.9.0.post0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->-r requirements.txt (line 2)) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->-r requirements.txt (line 2)) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->-r requirements.txt (line 2)) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->-r requirements.txt (line 2)) (4.67.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 1)) (1.17.0)\n",
            "Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
            "Installing collected packages: argparse\n",
            "Successfully installed argparse-1.4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "argparse"
                ]
              },
              "id": "5cb72289bf7d46dc8760703daa726d98"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pycocotools"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdY3vPU9R1Ux",
        "outputId": "d076cc25-66f2-41f8-e793-3d24c9c13a1c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.11/dist-packages (2.0.10)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pycocotools) (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3z4ZCKsSK_S",
        "outputId": "f51ad302-ac87-4d54-beca-f11852209b9d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p models\n",
        "!wget https://www.dropbox.com/s/ne0ixz5d58ccbbz/pretrained_model.zip?dl=1 -O pretrained_model.zip\n",
        "!unzip -q pretrained_model.zip -d models/\n",
        "!rm pretrained_model.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jy_-DWBzSce6",
        "outputId": "4dd27fc7-b718-40d8-f10a-002988d9b766"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-27 10:01:09--  https://www.dropbox.com/s/ne0ixz5d58ccbbz/pretrained_model.zip?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.81.18, 2620:100:6030:18::a27d:5012\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.81.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.dropbox.com/scl/fi/5pbpnmdqarpl3im03e6sk/pretrained_model.zip?rlkey=t60qk1iyys5fejbbwgvx5p5hq&dl=1 [following]\n",
            "--2025-06-27 10:01:09--  https://www.dropbox.com/scl/fi/5pbpnmdqarpl3im03e6sk/pretrained_model.zip?rlkey=t60qk1iyys5fejbbwgvx5p5hq&dl=1\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uccfc6ba63821f5b89641aaf6aa1.dl.dropboxusercontent.com/cd/0/inline/CsZShB-u2NU9DaO4fDrON3k7TP-vVC1whgNXFWDFhsTKFlpRiDNffe-KQTpOMM_troulqw3RbmnhI3OX0rGhFJmldAPwLj_RlNcuzhP0KY20prwisyifHCT-Cg6_V2_G_TbLbyXKmsnJ6nDqC_C1YZ0J/file?dl=1# [following]\n",
            "--2025-06-27 10:01:11--  https://uccfc6ba63821f5b89641aaf6aa1.dl.dropboxusercontent.com/cd/0/inline/CsZShB-u2NU9DaO4fDrON3k7TP-vVC1whgNXFWDFhsTKFlpRiDNffe-KQTpOMM_troulqw3RbmnhI3OX0rGhFJmldAPwLj_RlNcuzhP0KY20prwisyifHCT-Cg6_V2_G_TbLbyXKmsnJ6nDqC_C1YZ0J/file?dl=1\n",
            "Resolving uccfc6ba63821f5b89641aaf6aa1.dl.dropboxusercontent.com (uccfc6ba63821f5b89641aaf6aa1.dl.dropboxusercontent.com)... 162.125.85.15, 2620:100:6031:15::a27d:510f\n",
            "Connecting to uccfc6ba63821f5b89641aaf6aa1.dl.dropboxusercontent.com (uccfc6ba63821f5b89641aaf6aa1.dl.dropboxusercontent.com)|162.125.85.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/CsZjYO_Fin8VLUiwebvz--iXNq0j4oGoMgKkBFXM6qVvKWyH4CmjMjJ1UiFHW3AhUkJxIaCRQEESFqRjRIbZ3oeNkdsvCnq_3hFcS-nVrJ_rYWHg3F1dbZTYN899Z89hgfjN8h-kl18s79oqsALQgLZuidC7INTL7GVGGiTECK8krn0f0mHuViIJ9N3Ekp4SanSNOjf9apo4uzGvlM7DZQA9AcM1H3V73UXcDtzLAyLKUwqWIjmanjwEH4ynCgkwxvgzQFMc-uDCKqZ2QRMXLlu_Xugu0pIEExOm35hb22BV3ibuxgCYjbA2G3_WQobEC6VGoHdc4pTKeAhJJmjPimJlEUsRGexnABYfthNbochuFDz0w7E4twQ1t-6ZZuubI0M/file?dl=1 [following]\n",
            "--2025-06-27 10:01:12--  https://uccfc6ba63821f5b89641aaf6aa1.dl.dropboxusercontent.com/cd/0/inline2/CsZjYO_Fin8VLUiwebvz--iXNq0j4oGoMgKkBFXM6qVvKWyH4CmjMjJ1UiFHW3AhUkJxIaCRQEESFqRjRIbZ3oeNkdsvCnq_3hFcS-nVrJ_rYWHg3F1dbZTYN899Z89hgfjN8h-kl18s79oqsALQgLZuidC7INTL7GVGGiTECK8krn0f0mHuViIJ9N3Ekp4SanSNOjf9apo4uzGvlM7DZQA9AcM1H3V73UXcDtzLAyLKUwqWIjmanjwEH4ynCgkwxvgzQFMc-uDCKqZ2QRMXLlu_Xugu0pIEExOm35hb22BV3ibuxgCYjbA2G3_WQobEC6VGoHdc4pTKeAhJJmjPimJlEUsRGexnABYfthNbochuFDz0w7E4twQ1t-6ZZuubI0M/file?dl=1\n",
            "Reusing existing connection to uccfc6ba63821f5b89641aaf6aa1.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 253295231 (242M) [application/binary]\n",
            "Saving to: ‘pretrained_model.zip’\n",
            "\n",
            "pretrained_model.zi 100%[===================>] 241.56M  16.0MB/s    in 16s     \n",
            "\n",
            "2025-06-27 10:01:29 (15.2 MB/s) - ‘pretrained_model.zip’ saved [253295231/253295231]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p data\n",
        "!wget https://www.dropbox.com/s/26adb7y9m98uisa/vocap.zip?dl=1 -O vocap.zip\n",
        "!unzip -q vocap.zip -d data/\n",
        "!rm vocap.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLI5KYvAS74G",
        "outputId": "754be724-b4a4-4757-b623-63e71b0f718e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-27 10:01:45--  https://www.dropbox.com/s/26adb7y9m98uisa/vocap.zip?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.85.18, 2620:100:6030:18::a27d:5012\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.85.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.dropbox.com/scl/fi/r7g8pbh36tmcpbyk0gabm/vocap.zip?rlkey=xl8bmroltgedbq7m7glk3i57z&dl=1 [following]\n",
            "--2025-06-27 10:01:46--  https://www.dropbox.com/scl/fi/r7g8pbh36tmcpbyk0gabm/vocap.zip?rlkey=xl8bmroltgedbq7m7glk3i57z&dl=1\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucdd16b78b9e38c0bd4f6c725e60.dl.dropboxusercontent.com/cd/0/inline/CsbeUrEIX4THNXJKLQrU6IysyxGgKtcLtxSHiy_x6RE5ChdLUH_b4X8Fzd5m2bbOz69a129Pc8-yGhjryr9i9vT84zw03U1IOYZ5JxQZrfJMERZGcTwbUeuxEGCZ23Mc3jTE9OI3YP_sPGLgEnrnrdKb/file?dl=1# [following]\n",
            "--2025-06-27 10:01:47--  https://ucdd16b78b9e38c0bd4f6c725e60.dl.dropboxusercontent.com/cd/0/inline/CsbeUrEIX4THNXJKLQrU6IysyxGgKtcLtxSHiy_x6RE5ChdLUH_b4X8Fzd5m2bbOz69a129Pc8-yGhjryr9i9vT84zw03U1IOYZ5JxQZrfJMERZGcTwbUeuxEGCZ23Mc3jTE9OI3YP_sPGLgEnrnrdKb/file?dl=1\n",
            "Resolving ucdd16b78b9e38c0bd4f6c725e60.dl.dropboxusercontent.com (ucdd16b78b9e38c0bd4f6c725e60.dl.dropboxusercontent.com)... 162.125.85.15, 2620:100:6031:15::a27d:510f\n",
            "Connecting to ucdd16b78b9e38c0bd4f6c725e60.dl.dropboxusercontent.com (ucdd16b78b9e38c0bd4f6c725e60.dl.dropboxusercontent.com)|162.125.85.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/CsYBQEh0VxGoEVZ51_US4sqHOVCyeyKHTQ82hVkdOXgboqj0irNvOGNcq6p_PeK0Q82HHhe-CSA6sCuQ0QVtIWFYXMcyy6tdQWB0h8_kTCQ_aE7lDWd2eFloknELy-7sbVtppRp1aBw1gj6ntArYSAY88oUAYc-9QGDkyAmi5Q63qo8WBCeM6CLT_MfU6RtyO8xj3yoNBZHNE76pDugYRQ9ozdUdjVqIEdFAx0kNk58kxrMP743QK44KhLTrq-lVx6Wm9qXmxqLbxhFUL16N8wTl1KohxLiH4QH4lVF3njFA4DdLLlSj6XCUUNG0_CHx4LUsFNsJXdpk2gli7MY75YM6lCgIGZZf5ZF5sDbCdMGd80XBcjp1F7xiESWgaEOnjs4/file?dl=1 [following]\n",
            "--2025-06-27 10:01:47--  https://ucdd16b78b9e38c0bd4f6c725e60.dl.dropboxusercontent.com/cd/0/inline2/CsYBQEh0VxGoEVZ51_US4sqHOVCyeyKHTQ82hVkdOXgboqj0irNvOGNcq6p_PeK0Q82HHhe-CSA6sCuQ0QVtIWFYXMcyy6tdQWB0h8_kTCQ_aE7lDWd2eFloknELy-7sbVtppRp1aBw1gj6ntArYSAY88oUAYc-9QGDkyAmi5Q63qo8WBCeM6CLT_MfU6RtyO8xj3yoNBZHNE76pDugYRQ9ozdUdjVqIEdFAx0kNk58kxrMP743QK44KhLTrq-lVx6Wm9qXmxqLbxhFUL16N8wTl1KohxLiH4QH4lVF3njFA4DdLLlSj6XCUUNG0_CHx4LUsFNsJXdpk2gli7MY75YM6lCgIGZZf5ZF5sDbCdMGd80XBcjp1F7xiESWgaEOnjs4/file?dl=1\n",
            "Reusing existing connection to ucdd16b78b9e38c0bd4f6c725e60.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 140904 (138K) [application/binary]\n",
            "Saving to: ‘vocap.zip’\n",
            "\n",
            "vocap.zip           100%[===================>] 137.60K   222KB/s    in 0.6s    \n",
            "\n",
            "2025-06-27 10:01:49 (222 KB/s) - ‘vocap.zip’ saved [140904/140904]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import files\n",
        "print(\"Please choose the image file you want to caption:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print(f'User uploaded file \"{fn}\" with length {len(uploaded[fn])} bytes')\n",
        "\n",
        "image_filename = list(uploaded.keys())[0]\n",
        "print(f\"\\nWill generate caption for: {image_filename}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "yySHrdPPTQsj",
        "outputId": "1514ed3e-38ab-4da4-b872-8721322bae16"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please choose the image file you want to caption:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6aeb3420-f402-44d1-ab7e-a4cd7e1b7b0c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6aeb3420-f402-44d1-ab7e-a4cd7e1b7b0c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "list index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-7-2084042506.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'User uploaded file \"{fn}\" with length {len(uploaded[fn])} bytes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mimage_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muploaded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nWill generate caption for: {image_filename}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/pytorch-tutorial/tutorials/03-advanced/image_captioning/sample.py \\\n",
        "  --image={image_filename} \\\n",
        "  --encoder_path='./models/encoder-5-3000.pkl' \\\n",
        "  --decoder_path='./models/decoder-5-3000.pkl' \\\n",
        "  --vocab_path='./data/vocab.pkl'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H43v3kInU_KB",
        "outputId": "8641d830-6fc8-4186-fcfb-d0c83e55699d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet152-394f9c45.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-394f9c45.pth\n",
            "100% 230M/230M [00:01<00:00, 191MB/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/pytorch-tutorial/tutorials/03-advanced/image_captioning/sample.py\", line 81, in <module>\n",
            "    main(args)\n",
            "  File \"/content/pytorch-tutorial/tutorials/03-advanced/image_captioning/sample.py\", line 47, in main\n",
            "    image = load_image(args.image, transform)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/pytorch-tutorial/tutorials/03-advanced/image_captioning/sample.py\", line 17, in load_image\n",
            "    image = Image.open(image_path).convert('RGB')\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/PIL/Image.py\", line 3505, in open\n",
            "    fp = builtins.open(filename, \"rb\")\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '{image_filename}'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Replicate the Model Architecture in Keras\n",
        "\n",
        "build an equivalent model structure using Keras layers.\n",
        "\n",
        "Encoder (CNN):\n",
        "\n",
        "Use a Keras implementation of the same base CNN (e.g., tf.keras.applications.ResNet152).\n",
        "\n",
        "Load pre-trained ImageNet weights (weights='imagenet').\n",
        "\n",
        "Ensure the final classification layer is removed (include_top=False).\n",
        "\n",
        "Add appropriate pooling (pooling='avg') or flattening layers to get a feature vector.\n",
        "\n",
        "Add a Keras Dense layer (and potentially BatchNormalization if mimicking closely) to map the CNN features to the required embed_size.\n",
        "\n",
        "Decoder (RNN):\n",
        "\n",
        "Use Keras layers: keras.layers.Embedding, keras.layers.LSTM (or GRU), keras.layers.Dense.\n",
        "\n",
        "Crucially: Ensure the dimensions (e.g., vocab_size, embed_size, hidden_size for LSTM units) exactly match the PyTorch model's parameters.\n",
        "\n",
        "Replicate the mechanism for injecting the image features. In our Keras example code, we used the image features (passed through Dense layers) to generate the initial state for the LSTM decoder.\n",
        "\n",
        "2. Handle the Pre-trained PyTorch Weights\n",
        "\n",
        "a) Manual Weight Mapping (Fundamental but Tedious)\n",
        "\n",
        "Load PyTorch Weights: Use PyTorch to load the .pkl files and access the model's state_dict(), which is a dictionary mapping layer names to weight/bias tensors.\n",
        "\n",
        "Build Keras Model Instance: Create an instance of your Keras model defined in Step 1.\n",
        "\n",
        "Iterate and Match: Go through each layer in your Keras model (e.g., the Dense layer in the Encoder, the Embedding, LSTM, and Dense layers in the Decoder).\n",
        "\n",
        "Extract & Convert: For each Keras layer, find the corresponding weight tensor(s) in the PyTorch state_dict. Convert the PyTorch tensor(s) to NumPy arrays (e.g., using .cpu().numpy()).\n",
        "\n",
        "Handle Shape Differences: This is vital. Keras and PyTorch might store weights differently. For example, the kernel (weight matrix) of a Keras Dense layer often needs to be the transpose of the corresponding PyTorch Linear layer's weight matrix. You might need to use .T on the NumPy array. Convolutional layers can also have different dimension orders (channels_first vs. channels_last). You must carefully inspect and potentially reshape/transpose the NumPy arrays to match the Keras layer's expectations.\n",
        "\n",
        "Load into Keras: Use the Keras layer's set_weights([numpy_array_1, numpy_array_2, ...]) method to load the correctly shaped NumPy arrays. (Layers typically expect a list, e.g., [kernel_weights, bias_weights]).\n",
        "\n",
        "Gives complete control, guaranteed to work if architecture and shapes match perfectly.\n",
        "\n",
        "3. Align Vocabulary/Tokenizer\n",
        "\n",
        "You need to load the PyTorch vocab.pkl file.\n",
        "\n",
        "Create a Keras Tokenizer (e.g., tf.keras.preprocessing.text.Tokenizer) or implement a custom mapping.\n",
        "\n",
        "Ensure this Keras tokenizer uses the exact same word-to-index mapping, including the specific indices for special tokens like <start>, <pad>, and <end>, as defined in the PyTorch vocab.pkl. The generate_caption function relies on this consistency."
      ],
      "metadata": {
        "id": "rsin417PYmzr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class EncoderCNN(layers.Layer):\n",
        "    \"\"\"\n",
        "    Keras implementation of the Encoder CNN.\n",
        "    Loads a pre-trained ResNet-152 and maps its output features.\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_size):\n",
        "        super(EncoderCNN, self).__init__()\n",
        "        resnet = tf.keras.applications.ResNet152(\n",
        "            include_top=False,\n",
        "            weights='imagenet',\n",
        "            pooling='avg'\n",
        "        )\n",
        "        self.resnet_base = resnet\n",
        "\n",
        "        self.dense = layers.Dense(embed_size, activation='relu')\n",
        "\n",
        "    def call(self, images):\n",
        "        features = self.resnet_base(images)\n",
        "        features = self.dense(features)\n",
        "\n",
        "        return features\n",
        "\n",
        "\n",
        "embed_size = 256\n",
        "hidden_size = 512\n",
        "vocab_size = 10000\n",
        "max_seq_length = 20\n",
        "\n",
        "encoder_features_input = layers.Input(shape=(embed_size,), name='encoder_features')\n",
        "decoder_input = layers.Input(shape=(None,), name='decoder_input_indices')\n",
        "\n",
        "decoder_embedding_layer = layers.Embedding(input_dim=vocab_size,\n",
        "                                           output_dim=embed_size,\n",
        "                                           mask_zero=True,\n",
        "                                           name='decoder_embedding')\n",
        "\n",
        "decoder_lstm_layer = layers.LSTM(hidden_size,\n",
        "                                 return_sequences=True,\n",
        "                                 return_state=True,\n",
        "                                 name='decoder_lstm')\n",
        "\n",
        "decoder_dense_layer = layers.Dense(vocab_size, activation='softmax', name='decoder_output')\n",
        "\n",
        "initial_h_dense = layers.Dense(hidden_size, activation='relu', name='initial_h_dense')\n",
        "initial_c_dense = layers.Dense(hidden_size, activation='relu', name='initial_c_dense')\n",
        "\n",
        "initial_h_state = initial_h_dense(encoder_features_input)\n",
        "initial_c_state = initial_c_dense(encoder_features_input)\n",
        "initial_lstm_state = [initial_h_state, initial_c_state]\n",
        "\n",
        "embedded_captions = decoder_embedding_layer(decoder_input)\n",
        "lstm_outputs, _, _ = decoder_lstm_layer(embedded_captions,\n",
        "                                        initial_state=initial_lstm_state)\n",
        "decoder_outputs = decoder_dense_layer(lstm_outputs)\n",
        "\n",
        "training_model = keras.Model(inputs=[encoder_features_input, decoder_input],\n",
        "                             outputs=decoder_outputs,\n",
        "                             name='image_captioning_training_model')\n",
        "\n",
        "\n",
        "training_model.summary()\n",
        "\n",
        "\n",
        "inf_decoder_input_index = layers.Input(shape=(1,), name='inf_word_index')\n",
        "inf_prev_h_state = layers.Input(shape=(hidden_size,), name='inf_prev_h')\n",
        "inf_prev_c_state = layers.Input(shape=(hidden_size,), name='inf_prev_c')\n",
        "inf_prev_states = [inf_prev_h_state, inf_prev_c_state]\n",
        "\n",
        "inf_embedded_word = decoder_embedding_layer(inf_decoder_input_index)\n",
        "inf_lstm_outputs, inf_new_h, inf_new_c = decoder_lstm_layer(inf_embedded_word,\n",
        "                                                            initial_state=inf_prev_states)\n",
        "inf_decoder_outputs = decoder_dense_layer(inf_lstm_outputs)\n",
        "inf_new_states = [inf_new_h, inf_new_c]\n",
        "\n",
        "inference_decoder_model = keras.Model(\n",
        "    inputs=[inf_decoder_input_index] + inf_prev_states,\n",
        "    outputs=[inf_decoder_outputs] + inf_new_states,\n",
        "    name='image_captioning_inference_decoder'\n",
        ")\n",
        "\n",
        "\n",
        "inference_decoder_model.summary()\n",
        "\n",
        "\n",
        "def generate_caption(image_path, keras_encoder, inference_decoder_model, tokenizer, max_length):\n",
        "\n",
        "\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, (299, 299))\n",
        "    img = tf.keras.applications.resnet.preprocess_input(img)\n",
        "    img = tf.expand_dims(img, 0)\n",
        "\n",
        "    features = keras_encoder(img)\n",
        "\n",
        "    feature_to_state_model = keras.Model(encoder_features_input, initial_lstm_state)\n",
        "    current_h, current_c = feature_to_state_model.predict(features)\n",
        "\n",
        "    current_word_index = tf.constant([[tokenizer.word_index['<start>']]])\n",
        "    generated_indices = []\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        probs, current_h, current_c = inference_decoder_model.predict(\n",
        "            [current_word_index, current_h, current_c]\n",
        "        )\n",
        "\n",
        "        predicted_index = tf.argmax(probs[0, -1, :]).numpy()\n",
        "        generated_indices.append(predicted_index)\n",
        "\n",
        "        if predicted_index == tokenizer.word_index['<end>']:\n",
        "            break\n",
        "\n",
        "        current_word_index = tf.constant([[predicted_index]])\n",
        "\n",
        "    caption = tokenizer.sequences_to_texts([[idx for idx in generated_indices]])\n",
        "    return caption[0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 913
        },
        "id": "5U0eFQD3Vh4_",
        "outputId": "559501ee-7b2c-4985-8c37-18350b4c2dad"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"image_captioning_training_model\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"image_captioning_training_model\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ decoder_input_indi… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_features    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_embedding   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │  \u001b[38;5;34m2,560,000\u001b[0m │ decoder_input_in… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ initial_h_dense     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │    \u001b[38;5;34m131,584\u001b[0m │ encoder_features… │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ initial_c_dense     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │    \u001b[38;5;34m131,584\u001b[0m │ encoder_features… │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_lstm (\u001b[38;5;33mLSTM\u001b[0m) │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │  \u001b[38;5;34m1,574,912\u001b[0m │ decoder_embeddin… │\n",
              "│                     │ \u001b[38;5;34m512\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ initial_h_dense[\u001b[38;5;34m…\u001b[0m │\n",
              "│                     │ \u001b[38;5;34m512\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ initial_c_dense[\u001b[38;5;34m…\u001b[0m │\n",
              "│                     │ \u001b[38;5;34m512\u001b[0m)]             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_output      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │  \u001b[38;5;34m5,130,000\u001b[0m │ decoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │ \u001b[38;5;34m10000\u001b[0m)            │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ decoder_input_indi… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_features    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_embedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560,000</span> │ decoder_input_in… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ initial_h_dense     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │ encoder_features… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ initial_c_dense     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │ encoder_features… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>) │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> │ decoder_embeddin… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ initial_h_dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ initial_c_dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)]             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_output      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">5,130,000</span> │ decoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ <span style=\"color: #00af00; text-decoration-color: #00af00\">10000</span>)            │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m9,528,080\u001b[0m (36.35 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,528,080</span> (36.35 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m9,528,080\u001b[0m (36.35 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,528,080</span> (36.35 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"image_captioning_inference_decoder\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"image_captioning_inference_decoder\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ inf_word_index      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_embedding   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │  \u001b[38;5;34m2,560,000\u001b[0m │ inf_word_index[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ inf_prev_h          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ inf_prev_c          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_lstm (\u001b[38;5;33mLSTM\u001b[0m) │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m512\u001b[0m),  │  \u001b[38;5;34m1,574,912\u001b[0m │ decoder_embeddin… │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m),      │            │ inf_prev_h[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)]      │            │ inf_prev_c[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_output      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m10000\u001b[0m)  │  \u001b[38;5;34m5,130,000\u001b[0m │ decoder_lstm[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ inf_word_index      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_embedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560,000</span> │ inf_word_index[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ inf_prev_h          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ inf_prev_c          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>) │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>),  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> │ decoder_embeddin… │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>),      │            │ inf_prev_h[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)]      │            │ inf_prev_c[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_output      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10000</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">5,130,000</span> │ decoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m9,264,912\u001b[0m (35.34 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,264,912</span> (35.34 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m9,264,912\u001b[0m (35.34 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,264,912</span> (35.34 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Translating Between Different Languages (Japanese-English)\n",
        "\n",
        "Dataset: A large, high-quality parallel corpus (sentence-aligned Japanese and English text) is essential. Examples include JParaCrawl, KFTT, or ASPEC.\n",
        "\n",
        "Tokenization (Japanese): This is a key challenge due to the lack of spaces and multiple scripts (Hiragana, Katakana, Kanji).\n",
        "\n",
        "Morphological Analyzers: Tools like MeCab or Sudachi break text into meaningful words based on dictionaries.\n",
        "\n",
        "Subword Tokenization: Methods like SentencePiece (BPE or Unigram) are standard for NMT. They break words/characters into common sub-units, handling rare words and morphology effectively without relying solely on large dictionaries.\n",
        "\n",
        "Tokenization (English): While simpler, using a compatible subword tokenizer (like SentencePiece) often yields better results than basic space splitting, especially when paired with Japanese subword tokenization.\n",
        "\n",
        "Vocabulary: Build separate or joint vocabularies based on the chosen tokenization method. Subword methods help manage vocabulary size.\n",
        "\n",
        "Model Architecture: While the core Encoder-Decoder structure (LSTM, GRU, or Transformer) applies, hyperparameters might need tuning. The significant difference in sentence structure (Japanese SOV vs. English SVO) is a major challenge that modern NMT models learn to handle through training data.\n",
        "\n",
        "Handling Specifics: Japanese particles (like は, を, が), politeness levels, and omitted subjects require the model to learn complex grammatical mappings, which can be difficult. Neural Machine Translation (NMT) models have shown significant improvements over older methods in handling these complexities but still require large amounts of data.\n",
        "\n",
        "2. Advanced Methods of Machine Translation\n",
        "\n",
        "Attention Mechanisms:\n",
        "\n",
        "Concept: Allows the decoder to dynamically focus on relevant parts of the entire input (encoder hidden states) when generating each output word, rather than relying solely on the final encoder state.\n",
        "\n",
        "How: Calculates \"attention scores\" weighting the importance of each input word for the current output word. A context-specific vector is created as a weighted sum of encoder states.\n",
        "\n",
        "Benefits: Significantly improves long-sentence translation, better handling of word alignment. Common types include Bahdanau (additive) and Luong (multiplicative) attention.\n",
        "\n",
        "Transformer Models:\n",
        "\n",
        "Concept: Introduced in \"Attention Is All You Need,\" Transformers discard recurrence (LSTMs/GRUs) entirely and rely solely on attention mechanisms, primarily self-attention.\n",
        "\n",
        "Architecture: Uses stacked Encoder and Decoder layers. Key components include:\n",
        "\n",
        "Self-Attention: Allows each input token to weigh the importance of all other tokens within the same sequence (input or output).\n",
        "\n",
        "Multi-Head Attention: Runs multiple self-attention processes in parallel, allowing the model to capture different types of relationships simultaneously.\n",
        "\n",
        "Positional Encoding: Explicitly adds information about word order, as there are no sequential processing steps like in RNNs.\n",
        "\n",
        "Feed-Forward Networks: Standard layers applied independently at each position.\n",
        "\n",
        "Benefits: State-of-the-art performance, highly parallelizable (faster training), excellent at capturing long-range dependencies. Forms the foundation of most modern large language models (LLMs).\n",
        "\n",
        "3. Generating Images from Text (Text-to-Image Synthesis)\n",
        "\n",
        "Generative Adversarial Networks (GANs):\n",
        "\n",
        "How: Used a Generator network (creates images from text embeddings + noise) and a Discriminator network (judges realism). Text embeddings guided the Generator.\n",
        "\n",
        "Examples: StackGAN, AttnGAN (used attention to link words to image regions).\n",
        "\n",
        "Limitations: Training instability, difficulty achieving high fidelity and strong text alignment for complex prompts.\n",
        "\n",
        "Diffusion Models (Current State-of-the-Art):\n",
        "\n",
        "How: Learn to reverse a process of adding noise to images. To generate, they start with random noise and iteratively denoise it, guided by text embeddings.\n",
        "\n",
        "Text Conditioning: Powerful text encoders (like CLIP or T5) convert the prompt into vector representations. These vectors guide the denoising process at each step, often using cross-attention within the denoising network (typically a U-Net). Techniques like classifier-free guidance enhance prompt adherence.\n",
        "\n",
        "Examples: DALL-E 2/3, Stable Diffusion, Midjourney, Google Imagen.\n",
        "\n",
        "Benefits: Produce high-resolution, coherent, diverse images that strongly align with complex prompts. Have largely surpassed GANs in quality and control\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YR0NxZhbeNq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LMZy-yDJbs2z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}