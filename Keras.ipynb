{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NeshPk/NeshPk/blob/main/Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe071e5c",
      "metadata": {
        "id": "fe071e5c"
      },
      "source": [
        "##### [Problem 1] Sharing and executing the official tutorial model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7dc633ab",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "7dc633ab",
        "outputId": "474291dc-f99f-4551-ba26-60255129f446"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Clone the TensorFlow models repository\\ngit clone https://github.com/tensorflow/models.git\\ncd models/tutorials\\n\\n# Pick a model, for example, the official tutorial on MNIST\\ncd image/mnist\\n\\n# Install dependencies\\npip install -r requirements.txt\\n\\n# Run the example script (e.g., train a basic CNN on MNIST dataset)\\npython cnn_mnist.py\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "\"\"\"\n",
        "# Clone the TensorFlow models repository\n",
        "git clone https://github.com/tensorflow/models.git\n",
        "cd models/tutorials\n",
        "\n",
        "# Pick a model, for example, the official tutorial on MNIST\n",
        "cd image/mnist\n",
        "\n",
        "# Install dependencies\n",
        "pip install -r requirements.txt\n",
        "\n",
        "# Run the example script (e.g., train a basic CNN on MNIST dataset)\n",
        "python cnn_mnist.py\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89957422",
      "metadata": {
        "id": "89957422"
      },
      "source": [
        "##### [Problem 2] (Advance assignment) Execute various methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "aae64639",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "aae64639",
        "outputId": "eb917c7f-863d-476a-ce39-5e7599d5f88b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Clone the tensorflow models repository\\ngit clone https://github.com/tensorflow/models.git\\ncd models/research\\n\\n# Choose a model directory, for example, the object detection model\\ncd object_detection\\n\\n# Install dependencies\\npip install -r requirements.txt\\n\\n# Execute a script, for example, testing a pre-trained object detection model\\npython object_detection/model_main_tf2.py --pipeline_config_path=path_to_config_file.config\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "\"\"\"\n",
        "# Clone the tensorflow models repository\n",
        "git clone https://github.com/tensorflow/models.git\n",
        "cd models/research\n",
        "\n",
        "# Choose a model directory, for example, the object detection model\n",
        "cd object_detection\n",
        "\n",
        "# Install dependencies\n",
        "pip install -r requirements.txt\n",
        "\n",
        "# Execute a script, for example, testing a pre-trained object detection model\n",
        "python object_detection/model_main_tf2.py --pipeline_config_path=path_to_config_file.config\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c4d19bb",
      "metadata": {
        "id": "5c4d19bb"
      },
      "source": [
        "[Problem 3] Learning Iris (binary classification) with Keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c082a789",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c082a789",
        "outputId": "fae60d66-032f-48ff-e791-e2abcb1aa4be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3817 - loss: 1.0697\n",
            "Epoch 2/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3924 - loss: 0.9157  \n",
            "Epoch 3/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3781 - loss: 0.8357 \n",
            "Epoch 4/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3158 - loss: 0.7819  \n",
            "Epoch 5/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2863 - loss: 0.7356  \n",
            "Epoch 6/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5861 - loss: 0.7023 \n",
            "Epoch 7/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6819 - loss: 0.6774 \n",
            "Epoch 8/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6831 - loss: 0.6663 \n",
            "Epoch 9/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6780 - loss: 0.6705  \n",
            "Epoch 10/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6431 - loss: 0.6874 \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - accuracy: 0.7000 - loss: 0.6372\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6372233629226685, 0.699999988079071]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Convert the labels to binary (only for 'Iris-versicolor' and 'Iris-virginica')\n",
        "y = (y == 1).astype(int)  # Binary classification for class 1 vs class 2\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the Keras model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(10, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=16)\n",
        "\n",
        "# Evaluate the model\n",
        "model.evaluate(X_test, y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e8fc873",
      "metadata": {
        "id": "1e8fc873"
      },
      "source": [
        "##### [Problem 4] Learn Iris (multi-level classification) with Keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "0eb8e3fa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eb8e3fa",
        "outputId": "560c932c-6789-4279-8d07-4a495b5d4f51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3064 - loss: 3.8635  \n",
            "Epoch 2/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3668 - loss: 3.2059 \n",
            "Epoch 3/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3759 - loss: 2.8198 \n",
            "Epoch 4/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3206 - loss: 2.7579 \n",
            "Epoch 5/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2944 - loss: 2.7021  \n",
            "Epoch 6/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3206 - loss: 2.4269  \n",
            "Epoch 7/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1208 - loss: 2.2543  \n",
            "Epoch 8/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0288 - loss: 2.0513 \n",
            "Epoch 9/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1013 - loss: 2.0801     \n",
            "Epoch 10/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2238 - loss: 2.0010 \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.3667 - loss: 1.7500\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.7499523162841797, 0.36666667461395264]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the Keras model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(10, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')  # 3 classes\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=16)\n",
        "\n",
        "# Evaluate the model\n",
        "model.evaluate(X_test, y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55406034",
      "metadata": {
        "id": "55406034"
      },
      "source": [
        "##### [Problem 5] Learning House Prices with Keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "71b123c1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71b123c1",
        "outputId": "d6445206-a438-48d2-824f-f3fa4392beab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found train.csv in the current directory.\n",
            "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
            "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
            "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
            "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
            "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
            "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
            "\n",
            "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
            "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
            "1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n",
            "2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n",
            "3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
            "4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n",
            "\n",
            "  YrSold  SaleType  SaleCondition  SalePrice  \n",
            "0   2008        WD         Normal     208500  \n",
            "1   2007        WD         Normal     181500  \n",
            "2   2008        WD         Normal     223500  \n",
            "3   2006        WD        Abnorml     140000  \n",
            "4   2008        WD         Normal     250000  \n",
            "\n",
            "[5 rows x 81 columns]\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: nan\n",
            "Epoch 2/10\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan\n",
            "Epoch 3/10\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan\n",
            "Epoch 4/10\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan\n",
            "Epoch 5/10\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan\n",
            "Epoch 6/10\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan\n",
            "Epoch 7/10\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan\n",
            "Epoch 8/10\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan\n",
            "Epoch 9/10\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan\n",
            "Epoch 10/10\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: nan  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "nan"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import os # Import the os module\n",
        "\n",
        "# Load dataset\n",
        "# Check if the data directory exists, if not, check the current directory for train.csv\n",
        "data_path = 'data/train.csv'\n",
        "if not os.path.exists(data_path):\n",
        "    # If 'data/train.csv' is not found, try 'train.csv' in the current directory\n",
        "    data_path = 'train.csv'\n",
        "    if not os.path.exists(data_path):\n",
        "        # If 'train.csv' is also not found, raise an error or print a message\n",
        "        print(\"Error: train.csv not found in 'data/' or the current directory.\")\n",
        "        # Alternatively, you could raise a FileNotFoundError here:\n",
        "        # raise FileNotFoundError(\"train.csv not found in 'data/' or the current directory.\")\n",
        "    else:\n",
        "        print(\"Found train.csv in the current directory.\")\n",
        "else:\n",
        "    print(\"Found train.csv in 'data/' directory.\")\n",
        "\n",
        "# Load the dataset using the determined path\n",
        "df = pd.read_csv(data_path)\n",
        "\n",
        "# Display the first few rows to understand the structure of the data\n",
        "print(df.head())\n",
        "\n",
        "# Identify non-numeric columns that need encoding\n",
        "categorical_columns = df.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Encode categorical columns using LabelEncoder or One-Hot Encoding\n",
        "label_encoders = {}\n",
        "for column in categorical_columns:\n",
        "    # Handle potential NaN values before encoding\n",
        "    df[column] = df[column].astype(str)\n",
        "    if df[column].dtype == 'object':\n",
        "        le = LabelEncoder()\n",
        "        # Fit and transform, handling potential unseen labels during transform if necessary\n",
        "        # For simplicity here, we fit and transform on the entire column\n",
        "        df[column] = le.fit_transform(df[column])\n",
        "        label_encoders[column] = le\n",
        "\n",
        "# Now, df should have all numeric values (encoded) for categorical columns\n",
        "# Drop columns with potentially problematic data types or high cardinality if necessary\n",
        "# For this dataset, let's assume all encoded columns are usable.\n",
        "X = df.drop('SalePrice', axis=1).values\n",
        "y = df['SalePrice'].values\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define the Keras model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=16)\n",
        "\n",
        "# Evaluate the model\n",
        "model.evaluate(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9eb5fa26",
      "metadata": {
        "id": "9eb5fa26"
      },
      "source": [
        "##### [Problem 6] Learning MNIST with Keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "60759fca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60759fca",
        "outputId": "27e3cbaa-f893-4de0-b423-7cf9d9c8bae0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Epoch 1/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.8560 - loss: 0.5101\n",
            "Epoch 2/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.9591 - loss: 0.1415\n",
            "Epoch 3/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.9722 - loss: 0.0961\n",
            "Epoch 4/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9795 - loss: 0.0711\n",
            "Epoch 5/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9830 - loss: 0.0589\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9727 - loss: 0.0881\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.07579535245895386, 0.9768000245094299]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load dataset\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
        "X_train = X_train.reshape(-1, 28 * 28)\n",
        "X_test = X_test.reshape(-1, 28 * 28)\n",
        "\n",
        "# One-hot encode the labels\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Define the Keras model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=64)\n",
        "\n",
        "# Evaluate the model\n",
        "model.evaluate(X_test, y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f96600dc",
      "metadata": {
        "id": "f96600dc"
      },
      "source": [
        "##### [Problem 7] (Advance assignment) Rewriting to PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "ccdfa77c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccdfa77c",
        "outputId": "fef5a054-0cd0-49f6-e04b-12081431850b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.817531406879425\n",
            "Epoch 2/10, Loss: 0.6792219281196594\n",
            "Epoch 3/10, Loss: 0.8047783970832825\n",
            "Epoch 4/10, Loss: 0.7891013622283936\n",
            "Epoch 5/10, Loss: 0.7109968066215515\n",
            "Epoch 6/10, Loss: 0.6464657187461853\n",
            "Epoch 7/10, Loss: 0.6640101075172424\n",
            "Epoch 8/10, Loss: 0.6540514826774597\n",
            "Epoch 9/10, Loss: 0.6926321983337402\n",
            "Epoch 10/10, Loss: 0.6954423189163208\n",
            "Accuracy: 46.67%\n"
          ]
        }
      ],
      "source": [
        "#1. Iris Binary Classification with PyTorch\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "y = (y == 1).astype(int)  # Binary classification for 'Iris-versicolor' vs 'Iris-virginica'\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "# Create DataLoader\n",
        "train_data = TensorDataset(X_train, y_train)\n",
        "test_data = TensorDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=16, shuffle=False)\n",
        "\n",
        "# Define the model\n",
        "class IrisModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(IrisModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(4, 10)\n",
        "        self.fc2 = nn.Linear(10, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "model = IrisModel()\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(X_batch)\n",
        "        loss = criterion(output, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "# Evaluate the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        outputs = model(X_batch)\n",
        "        predicted = (outputs > 0.5).float()\n",
        "        total += y_batch.size(0)\n",
        "        correct += (predicted == y_batch).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "93dd2be0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93dd2be0",
        "outputId": "dbf9028f-4786-401f-b73d-8fbb4fbde8e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 1.2845590114593506\n",
            "Epoch 2/10, Loss: 1.2163459062576294\n",
            "Epoch 3/10, Loss: 1.0576571226119995\n",
            "Epoch 4/10, Loss: 1.145279049873352\n",
            "Epoch 5/10, Loss: 1.1159659624099731\n",
            "Epoch 6/10, Loss: 0.9498445987701416\n",
            "Epoch 7/10, Loss: 1.0538406372070312\n",
            "Epoch 8/10, Loss: 1.019718050956726\n",
            "Epoch 9/10, Loss: 1.0466504096984863\n",
            "Epoch 10/10, Loss: 0.8998193740844727\n",
            "Accuracy: 36.67%\n"
          ]
        }
      ],
      "source": [
        "#2. Iris Multi-Class Classification with PyTorch\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Create DataLoader\n",
        "train_data = TensorDataset(X_train, y_train)\n",
        "test_data = TensorDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=16, shuffle=False)\n",
        "\n",
        "# Define the model\n",
        "class IrisModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(IrisModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(4, 10)\n",
        "        self.fc2 = nn.Linear(10, 3)  # 3 classes for multi-class classification\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "model = IrisModel()\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(X_batch)\n",
        "        loss = criterion(output, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "# Evaluate the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        outputs = model(X_batch)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += y_batch.size(0)\n",
        "        correct += (predicted == y_batch).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "69cca472",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69cca472",
        "outputId": "32e438fe-0c63-450b-fcfd-f640c0959f18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
            "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
            "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
            "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
            "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
            "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
            "\n",
            "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
            "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
            "1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n",
            "2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n",
            "3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
            "4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n",
            "\n",
            "  YrSold  SaleType  SaleCondition  SalePrice  \n",
            "0   2008        WD         Normal     208500  \n",
            "1   2007        WD         Normal     181500  \n",
            "2   2008        WD         Normal     223500  \n",
            "3   2006        WD        Abnorml     140000  \n",
            "4   2008        WD         Normal     250000  \n",
            "\n",
            "[5 rows x 81 columns]\n",
            "Epoch 1/10, Loss: nan\n",
            "Epoch 2/10, Loss: nan\n",
            "Epoch 3/10, Loss: nan\n",
            "Epoch 4/10, Loss: nan\n",
            "Epoch 5/10, Loss: nan\n",
            "Epoch 6/10, Loss: nan\n",
            "Epoch 7/10, Loss: nan\n",
            "Epoch 8/10, Loss: nan\n",
            "Epoch 9/10, Loss: nan\n",
            "Epoch 10/10, Loss: nan\n",
            "Test Loss: nan\n"
          ]
        }
      ],
      "source": [
        "#3.House Prices Regression with PyTorch\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('train.csv')\n",
        "\n",
        "# Display the first few rows to understand the structure of the data\n",
        "print(df.head())\n",
        "\n",
        "# Preprocessing: Handle categorical variables\n",
        "df = pd.get_dummies(df, drop_first=True)\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop('SalePrice', axis=1).values\n",
        "y = df['SalePrice'].values\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "# Define the model\n",
        "class HousePriceModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(HousePriceModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(X_train.shape[1], 64)\n",
        "        self.fc2 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "model = HousePriceModel()\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    output = model(X_train)\n",
        "    loss = criterion(output, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "# Evaluate the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred = model(X_test)\n",
        "    loss = criterion(y_pred, y_test)\n",
        "    print(f\"Test Loss: {loss.item()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "25730b3b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25730b3b",
        "outputId": "c33bef76-ca41-4f96-d475-f34ec249ac8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 0.6723357009887695\n",
            "Epoch 2/20, Loss: 0.573206158876419\n",
            "Epoch 3/20, Loss: 0.4378126347064972\n",
            "Epoch 4/20, Loss: 0.355757497549057\n",
            "Epoch 5/20, Loss: 0.32165760099887847\n",
            "Epoch 6/20, Loss: 0.30208787858486175\n",
            "Epoch 7/20, Loss: 0.2913589465618134\n",
            "Epoch 8/20, Loss: 0.28192804038524627\n",
            "Epoch 9/20, Loss: 0.274800109565258\n",
            "Epoch 10/20, Loss: 0.2688262692093849\n",
            "Epoch 11/20, Loss: 0.2612754845619202\n",
            "Epoch 12/20, Loss: 0.2518048244714737\n",
            "Epoch 13/20, Loss: 0.24555717080831527\n",
            "Epoch 14/20, Loss: 0.23873199731111527\n",
            "Epoch 15/20, Loss: 0.23064703702926637\n",
            "Epoch 16/20, Loss: 0.22385681241750718\n",
            "Epoch 17/20, Loss: 0.21605511367321015\n",
            "Epoch 18/20, Loss: 0.20951101988554\n",
            "Epoch 19/20, Loss: 0.20228311002254487\n",
            "Epoch 20/20, Loss: 0.19722251668572427\n",
            "Accuracy on test set: 84.50%\n"
          ]
        }
      ],
      "source": [
        "#Binary Classification with a Custom Neural Network\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Create a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
        "\n",
        "# Step 2: Preprocess the dataset\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Convert the data into PyTorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)  # Reshape to match output\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "# Step 3: Create DataLoader\n",
        "train_data = TensorDataset(X_train, y_train)\n",
        "test_data = TensorDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
        "\n",
        "# Step 4: Define the custom neural network model\n",
        "class BinaryClassificationModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(BinaryClassificationModel, self).__init__()\n",
        "        # Define layers\n",
        "        self.fc1 = nn.Linear(input_dim, 64)  # First hidden layer\n",
        "        self.fc2 = nn.Linear(64, 32)         # Second hidden layer\n",
        "        self.fc3 = nn.Linear(32, 1)          # Output layer\n",
        "        self.sigmoid = nn.Sigmoid()          # Sigmoid activation for binary classification\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))   # Apply ReLU activation to the first hidden layer\n",
        "        x = torch.relu(self.fc2(x))   # Apply ReLU activation to the second hidden layer\n",
        "        x = self.fc3(x)               # Output layer\n",
        "        x = self.sigmoid(x)           # Sigmoid activation for output (binary classification)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "model = BinaryClassificationModel(input_dim=X_train.shape[1])\n",
        "\n",
        "# Step 5: Define loss function and optimizer\n",
        "criterion = nn.BCELoss()  # Binary Cross-Entropy loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Step 6: Train the model\n",
        "epochs = 20\n",
        "for epoch in range(epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        optimizer.zero_grad()        # Zero the gradients\n",
        "        outputs = model(X_batch)     # Forward pass\n",
        "        loss = criterion(outputs, y_batch)  # Calculate loss\n",
        "        loss.backward()              # Backpropagation\n",
        "        optimizer.step()             # Update weights\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Print the loss every epoch\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "# Step 7: Evaluate the model\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        outputs = model(X_batch)  # Forward pass\n",
        "        predicted = (outputs > 0.5).float()  # Convert to binary predictions (0 or 1)\n",
        "        total += y_batch.size(0)\n",
        "        correct += (predicted == y_batch).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f\"Accuracy on test set: {accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63c1b997",
      "metadata": {
        "id": "63c1b997"
      },
      "source": [
        "##### [Problem 8] (Advance assignment) Comparison of frameworks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32ba82b9",
      "metadata": {
        "id": "32ba82b9"
      },
      "source": [
        "# Comparison of Frameworks: TensorFlow vs. Keras vs. PyTorch\n",
        "\n",
        "## 1. Ease of Use and Learning Curve\n",
        "\n",
        "- **TensorFlow**:\n",
        "  - Originally had a steep learning curve.\n",
        "  - **TensorFlow 2.x** improved usability with eager execution and integration with Keras.\n",
        "  - Comprehensive documentation, but can be overwhelming for beginners.\n",
        "\n",
        "- **Keras (tf.keras)**:\n",
        "  - High-level API built on TensorFlow.\n",
        "  - Extremely user-friendly and modular.\n",
        "  - Ideal for quick prototyping and beginners.\n",
        "\n",
        "- **PyTorch**:\n",
        "  - Pythonic and intuitive API.\n",
        "  - Easier to learn, especially for those familiar with Python.\n",
        "  - Designed with dynamic graphs, making it easy to debug and modify models during training.\n",
        "\n",
        "## 2. Performance and Scalability\n",
        "\n",
        "- **TensorFlow**:\n",
        "  - Optimized for scalability, especially in production.\n",
        "  - Strong support for distributed training on multiple GPUs and TPUs.\n",
        "  - TensorFlow Lite for mobile and embedded systems.\n",
        "\n",
        "- **Keras (tf.keras)**:\n",
        "  - Benefits from TensorFlow’s performance and scalability.\n",
        "  - Ideal for prototyping but can scale with TensorFlow's distributed training tools.\n",
        "\n",
        "- **PyTorch**:\n",
        "  - Fast and dynamic computation graphs (define-by-run).\n",
        "  - Decent scalability, with multi-GPU support via `torch.distributed`.\n",
        "  - Newer tools for deployment like **TorchServe**, but not as mature as TensorFlow’s offerings.\n",
        "\n",
        "## 3. Flexibility and Control\n",
        "\n",
        "- **TensorFlow**:\n",
        "  - Offers high flexibility with its low-level API.\n",
        "  - Complete control over computation graphs for advanced use cases.\n",
        "\n",
        "- **Keras (tf.keras)**:\n",
        "  - Less flexible compared to TensorFlow’s low-level API.\n",
        "  - Easier to use but not as customizable for complex models.\n",
        "\n",
        "- **PyTorch**:\n",
        "  - Maximum flexibility with dynamic computation graphs.\n",
        "  - Ideal for research where model modification is frequent.\n",
        "\n",
        "## 4. Deployment and Support\n",
        "\n",
        "- **TensorFlow**:\n",
        "  - Extensive deployment tools: TensorFlow Serving, TensorFlow Lite, TensorFlow.js, TensorFlow Extended (TFX).\n",
        "  - Strong support for cloud services and mobile deployment.\n",
        "\n",
        "- **Keras (tf.keras)**:\n",
        "  - Inherits TensorFlow’s deployment capabilities.\n",
        "  - Limited to TensorFlow tools but simplifies deployment via a high-level interface.\n",
        "\n",
        "- **PyTorch**:\n",
        "  - Improving deployment tools: **TorchServe** for model serving, **TorchScript** for production optimization.\n",
        "  - More flexible, but less mature than TensorFlow in large-scale enterprise deployments.\n",
        "\n",
        "## 5. Community and Ecosystem\n",
        "\n",
        "- **TensorFlow**:\n",
        "  - Large community with strong industry backing (Google).\n",
        "  - Rich ecosystem: TensorFlow Hub, TensorFlow Lite, TensorFlow Extended (TFX).\n",
        "\n",
        "- **Keras (tf.keras)**:\n",
        "  - Strong community, particularly for beginners and intermediate users.\n",
        "  - Benefits from TensorFlow’s ecosystem.\n",
        "\n",
        "- **PyTorch**:\n",
        "  - Rapidly growing community, particularly in academia.\n",
        "  - Expanding ecosystem: Hugging Face Transformers, TorchServe, etc.\n",
        "\n",
        "## 6. Popularity and Adoption\n",
        "\n",
        "- **TensorFlow**:\n",
        "  - Widely adopted in both research and production, especially in large enterprises.\n",
        "  - Extensive use in cloud services (Google Cloud AI).\n",
        "\n",
        "- **Keras (tf.keras)**:\n",
        "  - Extremely popular for quick model prototyping.\n",
        "  - Used heavily in both academia and industry due to its ease of use.\n",
        "\n",
        "- **PyTorch**:\n",
        "  - Very popular in academia, and rapidly gaining adoption in the industry.\n",
        "  - Preferred for research, with increasing enterprise usage.\n",
        "\n",
        "## Summary Comparison Table\n",
        "\n",
        "| Feature               | **TensorFlow**                      | **Keras (tf.keras)**              | **PyTorch**                         |\n",
        "|-----------------------|-------------------------------------|----------------------------------|-------------------------------------|\n",
        "| **Ease of Use**       | Moderate to Advanced (improving)    | Very Easy (High-Level API)       | Very Easy (Pythonic API, Intuitive) |\n",
        "| **Performance**       | High (Optimized for Large-Scale)    | Depends on TensorFlow backend    | High (Dynamic graph, fast computation)|\n",
        "| **Flexibility**       | High (Low-Level Access)             | Limited (High-Level API)         | Very High (Dynamic Graph, flexible) |\n",
        "| **Scalability**       | Excellent (Multi-GPU, TPUs)         | Limited (Uses TensorFlow backend) | Good (Multi-GPU, TorchServe)        |\n",
        "| **Deployment**        | Excellent (TensorFlow Serving, Lite, etc.) | Inherits TensorFlow’s deployment tools | Improving (TorchServe, TorchScript) |\n",
        "| **Community**         | Large (Industry Focus)              | Strong (Beginner/Intermediate)   | Growing (Research Focus)           |\n",
        "| **Adoption**          | Widely adopted in industry          | Widely adopted in both research and industry | Growing rapidly in research and industry |\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}