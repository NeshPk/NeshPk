{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NeshPk/NeshPk/blob/main/LinearRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f06c34ab",
      "metadata": {
        "id": "f06c34ab"
      },
      "outputs": [],
      "source": [
        "class ScratchLinearRegression():\n",
        "    \"\"\"\n",
        "    線形回帰のスクラッチ実装\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    num_iter : int\n",
        "      イテレーション数\n",
        "    lr : float\n",
        "      学習率\n",
        "    no_bias : bool\n",
        "      バイアス項を入れない場合はTrue\n",
        "    verbose : bool\n",
        "      学習過程を出力する場合はTrue\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    self.coef_ : 次の形のndarray, shape (n_features,)\n",
        "      パラメータ\n",
        "    self.loss : 次の形のndarray, shape (self.iter,)\n",
        "      訓練データに対する損失の記録\n",
        "    self.val_loss : 次の形のndarray, shape (self.iter,)\n",
        "      検証データに対する損失の記録\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_iter, lr, no_bias, verbose):\n",
        "        # ハイパーパラメータを属性として記録\n",
        "        self.iter = num_iter\n",
        "        self.lr = lr\n",
        "        self.no_bias = no_bias\n",
        "        self.verbose = verbose\n",
        "        # 損失を記録する配列を用意\n",
        "        self.loss = np.zeros(self.iter)\n",
        "        self.val_loss = np.zeros(self.iter)\n",
        "\n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        \"\"\"\n",
        "        線形回帰を学習する。検証データが入力された場合はそれに対する損失と精度もイテレーションごとに計算する。\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 次の形のndarray, shape (n_samples, n_features)\n",
        "            訓練データの特徴量\n",
        "        y : 次の形のndarray, shape (n_samples, )\n",
        "            訓練データの正解値\n",
        "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
        "            検証データの特徴量\n",
        "        y_val : 次の形のndarray, shape (n_samples, )\n",
        "            検証データの正解値\n",
        "        \"\"\"\n",
        "        if self.verbose:\n",
        "            #verboseをTrueにした際は学習過程を出力\n",
        "            print()\n",
        "        pass\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        線形回帰を使い推定する。\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 次の形のndarray, shape (n_samples, n_features)\n",
        "            サンプル\n",
        "        Returns\n",
        "        -------\n",
        "            次の形のndarray, shape (n_samples, 1)\n",
        "            線形回帰による推定結果\n",
        "        \"\"\"\n",
        "        pass\n",
        "        return"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd10d1c9",
      "metadata": {
        "id": "bd10d1c9"
      },
      "source": [
        "##### 【Problem 1 】 Assumption function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "74559330",
      "metadata": {
        "id": "74559330"
      },
      "outputs": [],
      "source": [
        "def _linear_hypothesis(self, X):\n",
        "    \"\"\"\n",
        "    線形の仮定関数を計算する\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 次の形のndarray, shape (n_samples, n_features)\n",
        "      訓練データ\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    次の形のndarray, shape (n_samples,)\n",
        "      線形の仮定関数による推定結果\n",
        "    \"\"\"\n",
        "    # Add bias term if needed\n",
        "    if not self.no_bias:\n",
        "        X = np.c_[np.ones(X.shape[0]), X]\n",
        "\n",
        "    return X @ self.coef_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b0100a6",
      "metadata": {
        "id": "8b0100a6"
      },
      "source": [
        "##### 【Problem 2 】 Fastest descent method"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d54641f",
      "metadata": {
        "id": "3d54641f"
      },
      "source": [
        "1. _gradient_descent() Method Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d1f63fcc",
      "metadata": {
        "id": "d1f63fcc"
      },
      "outputs": [],
      "source": [
        "def _gradient_descent(self, X, error):\n",
        "    \"\"\"\n",
        "    勾配降下法によってパラメータを更新する\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : ndarray, shape (n_samples, n_features or n_features+1 if bias included)\n",
        "        訓練データの特徴量（バイアス項を含む可能性あり）\n",
        "\n",
        "    error : ndarray, shape (n_samples,)\n",
        "        予測値と正解値の誤差（residual）\n",
        "    \"\"\"\n",
        "    n_samples = X.shape[0]\n",
        "\n",
        "    # 勾配を計算（平均二乗誤差の勾配）\n",
        "    grad = (1 / n_samples) * X.T @ error\n",
        "\n",
        "    # パラメータを更新\n",
        "    self.coef_ -= self.lr * grad\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4a6bb95",
      "metadata": {
        "id": "e4a6bb95"
      },
      "source": [
        "2. Updated fit() Method Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "544481ae",
      "metadata": {
        "id": "544481ae"
      },
      "outputs": [],
      "source": [
        "def fit(self, X, y, X_val=None, y_val=None):\n",
        "    \"\"\"\n",
        "    線形回帰を学習する。検証データが入力された場合はそれに対する損失も計算する。\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : ndarray, shape (n_samples, n_features)\n",
        "        訓練データの特徴量\n",
        "\n",
        "    y : ndarray, shape (n_samples,)\n",
        "        訓練データの正解値\n",
        "\n",
        "    X_val : ndarray, shape (n_samples, n_features), optional\n",
        "        検証データの特徴量\n",
        "\n",
        "    y_val : ndarray, shape (n_samples,), optional\n",
        "        検証データの正解値\n",
        "    \"\"\"\n",
        "    n_samples = X.shape[0]\n",
        "\n",
        "    # バイアス項を加える（必要な場合）\n",
        "    if not self.no_bias:\n",
        "        X = np.c_[np.ones(n_samples), X]\n",
        "        if X_val is not None:\n",
        "            X_val = np.c_[np.ones(X_val.shape[0]), X_val]\n",
        "\n",
        "    # パラメータ初期化（0で初期化）\n",
        "    self.coef_ = np.zeros(X.shape[1])\n",
        "\n",
        "    for i in range(self.iter):\n",
        "        # 仮定関数による予測\n",
        "        y_pred = self._linear_hypothesis(X)\n",
        "\n",
        "        # 損失（誤差）を計算\n",
        "        error = y_pred - y\n",
        "\n",
        "        # パラメータ更新\n",
        "        self._gradient_descent(X, error)\n",
        "\n",
        "        # 訓練損失を記録\n",
        "        self.loss[i] = np.mean(error**2)\n",
        "\n",
        "        # 検証損失があれば記録\n",
        "        if X_val is not None and y_val is not None:\n",
        "            val_pred = self._linear_hypothesis(X_val)\n",
        "            val_error = val_pred - y_val\n",
        "            self.val_loss[i] = np.mean(val_error**2)\n",
        "\n",
        "        # 出力（verbose=Trueのときのみ）\n",
        "        if self.verbose and i % 10 == 0:\n",
        "            print(f\"Iteration {i}: Training Loss = {self.loss[i]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "441f6d7c",
      "metadata": {
        "id": "441f6d7c"
      },
      "source": [
        "3. Minor Fix to predict() Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a0e998c0",
      "metadata": {
        "id": "a0e998c0"
      },
      "outputs": [],
      "source": [
        "def predict(self, X):\n",
        "    \"\"\"\n",
        "    線形回帰を使い推定する\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : ndarray, shape (n_samples, n_features)\n",
        "        サンプル\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    ndarray, shape (n_samples,)\n",
        "        線形回帰による推定結果\n",
        "    \"\"\"\n",
        "    if not self.no_bias:\n",
        "        X = np.c_[np.ones(X.shape[0]), X]\n",
        "    return self._linear_hypothesis(X)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4941b9d",
      "metadata": {
        "id": "b4941b9d"
      },
      "source": [
        "##### 【Problem 3 】 Estimated"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ab7319e",
      "metadata": {
        "id": "3ab7319e"
      },
      "source": [
        "Final Implementation of predict()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "688dd343",
      "metadata": {
        "id": "688dd343"
      },
      "outputs": [],
      "source": [
        "def predict(self, X):\n",
        "    \"\"\"\n",
        "    線形回帰モデルによる推定を行う\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : ndarray, shape (n_samples, n_features)\n",
        "        推定したいサンプルデータ\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    ndarray, shape (n_samples,)\n",
        "        推定結果（予測値）\n",
        "    \"\"\"\n",
        "    return self._linear_hypothesis(X)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9019ec4f",
      "metadata": {
        "id": "9019ec4f"
      },
      "source": [
        "Reminder: _linear_hypothesis()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "92baea87",
      "metadata": {
        "id": "92baea87"
      },
      "outputs": [],
      "source": [
        "def _linear_hypothesis(self, X):\n",
        "    if not self.no_bias:\n",
        "        X = np.c_[np.ones(X.shape[0]), X]\n",
        "    return X @ self.coef_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2912c37a",
      "metadata": {
        "id": "2912c37a"
      },
      "source": [
        "##### 【Problem 4 】 Average square error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "6143d19c",
      "metadata": {
        "id": "6143d19c"
      },
      "outputs": [],
      "source": [
        "def MSE(y_pred, y):\n",
        "    \"\"\"\n",
        "    平均二乗誤差の計算\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y_pred : ndarray, shape (n_samples,)\n",
        "        推定した値\n",
        "\n",
        "    y : ndarray, shape (n_samples,)\n",
        "        正解値\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    mse : float\n",
        "        平均二乗誤差\n",
        "    \"\"\"\n",
        "    mse = np.mean((y_pred - y) ** 2)\n",
        "    return mse\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a8ba1fe",
      "metadata": {
        "id": "6a8ba1fe"
      },
      "source": [
        "【Problem 5 】 Purpose function"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d1e249f",
      "metadata": {
        "id": "4d1e249f"
      },
      "source": [
        "Step 1: Define a general-purpose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "6b693e72",
      "metadata": {
        "id": "6b693e72"
      },
      "outputs": [],
      "source": [
        "def half_MSE(y_pred, y):\n",
        "    \"\"\"\n",
        "    目的関数（1/2 平均二乗誤差）の計算\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y_pred : ndarray, shape (n_samples,)\n",
        "        推定値\n",
        "\n",
        "    y : ndarray, shape (n_samples,)\n",
        "        正解値\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    loss : float\n",
        "        目的関数の値（1/2 MSE）\n",
        "    \"\"\"\n",
        "    m = len(y)\n",
        "    return 0.5 * np.mean((y_pred - y) ** 2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d69b00f",
      "metadata": {
        "id": "1d69b00f"
      },
      "source": [
        "Step 2: Modify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "f24f0125",
      "metadata": {
        "id": "f24f0125"
      },
      "outputs": [],
      "source": [
        "class ScratchLinearRegression:\n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        if not self.no_bias:\n",
        "            X = np.c_[np.ones(X.shape[0]), X]\n",
        "            if X_val is not None:\n",
        "                X_val = np.c_[np.ones(X_val.shape[0]), X_val]\n",
        "\n",
        "        self.coef_ = np.zeros(X.shape[1])\n",
        "\n",
        "        for i in range(self.iter):\n",
        "            y_pred = self._linear_hypothesis(X)\n",
        "            error = y_pred - y\n",
        "\n",
        "            # Update parameters\n",
        "            self._gradient_descent(X, error)\n",
        "\n",
        "            # Record training loss\n",
        "            self.loss[i] = half_MSE(y_pred, y)\n",
        "\n",
        "            # Record validation loss if available\n",
        "            if X_val is not None and y_val is not None:\n",
        "                val_pred = self._linear_hypothesis(X_val)\n",
        "                self.val_loss[i] = half_MSE(val_pred, y_val)\n",
        "\n",
        "            if self.verbose and i % 10 == 0:\n",
        "                print(f\"Iteration {i}: Training Loss = {self.loss[i]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc0163d5",
      "metadata": {
        "id": "bc0163d5"
      },
      "source": [
        "【Problem 6 】 Learning and estimation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebcef395",
      "metadata": {
        "id": "ebcef395"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "8e714365",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e714365",
        "outputId": "bf4ec752-acb8-4420-c661-c2d1bdbccc98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0: Training Loss = 19442791762.8549\n",
            "Iteration 10: Training Loss = 21453823019589954075076246355121799168.0000\n",
            "Iteration 20: Training Loss = 42081609754241801104173429145616313941771170609993982305793736704.0000\n",
            "Iteration 30: Training Loss = 82542951803568001555595891590461232087468254481937872825117235541305109135439133970880528384.0000\n",
            "Iteration 40: Training Loss = 161907753344900711300996917481144187525570485852697277196345627103682243616116323645942367516633786169343222572600262656.0000\n",
            "Iteration 50: Training Loss = 317581574445952305366208688041963476205674371995587679795405823579745969047241269408992717981731675548835913968861772439589937636504371513062850560.0000\n",
            "Iteration 60: Training Loss = 622935309421028298796807294827299114030391467055833071692592126080044723617237194543426152699959936595259722984526396604316287473080687277999485328704175150250000715417124864.0000\n",
            "Iteration 70: Training Loss = 1221885748253673909613971565552367369076459658604717782025260740687562627515896159314092500520699958790587478890539153143491941306273619408101381972498598575708122120570412611198364803203772722572165120.0000\n",
            "Iteration 80: Training Loss = 2396725244509065019867951804392610720895207419570562580065639584339257026156945967537254421263924759650731366156800655126882727196060542231808705117631562503677861013860980128657265311199282941954093362789912497237856939862392832.0000\n",
            "Iteration 90: Training Loss = 4701169406285974687797508760959875034279646647687814395050891607657097720422490475122456336438197160753181410732434198743775987739392645812983887140951607597508744749651023847265573523522865343774263495901910295090114169993848989947035007760959233501167616.0000\n",
            "Iteration 100: Training Loss = 9221329744505761567079965888430062997492742469984488569868955406040952591949964659701713149345461241535422816017617863903763942661523464425294257317128934232510538919747753668448148415426661014629017392306060543731430265476036572598316960525388301617311899935700901915880402513297408.0000\n",
            "Iteration 110: Training Loss = inf\n",
            "Iteration 120: Training Loss = inf\n",
            "Iteration 130: Training Loss = inf\n",
            "Iteration 140: Training Loss = inf\n",
            "Iteration 150: Training Loss = inf\n",
            "Iteration 160: Training Loss = inf\n",
            "Iteration 170: Training Loss = inf\n",
            "Iteration 180: Training Loss = inf\n",
            "Iteration 190: Training Loss = inf\n",
            "\n",
            "--- Comparison ---\n",
            "Scratch RMSE: inf\n",
            "Scikit-learn RMSE: 36061.39606737877\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-fcea721491be>:56: RuntimeWarning: overflow encountered in square\n",
            "  return 0.5 * np.mean((y_pred - y_true) ** 2)\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/_core/_methods.py:127: RuntimeWarning: overflow encountered in reduce\n",
            "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_regression.py:570: RuntimeWarning: overflow encountered in square\n",
            "  output_errors = _average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# ========== Scratch Linear Regression Class ==========\n",
        "\n",
        "class ScratchLinearRegression:\n",
        "    def __init__(self, num_iter=100, lr=0.01, no_bias=False, verbose=False):\n",
        "        self.iter = num_iter\n",
        "        self.lr = lr\n",
        "        self.no_bias = no_bias\n",
        "        self.verbose = verbose\n",
        "        self.loss = np.zeros(self.iter)\n",
        "        self.val_loss = np.zeros(self.iter)\n",
        "\n",
        "    def _linear_hypothesis(self, X):\n",
        "        return X @ self.coef_\n",
        "\n",
        "    def _gradient_descent(self, X, error):\n",
        "        m = X.shape[0]\n",
        "        gradient = (X.T @ error) / m\n",
        "        self.coef_ -= self.lr * gradient\n",
        "\n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        if not self.no_bias:\n",
        "            X = np.c_[np.ones(X.shape[0]), X]\n",
        "            if X_val is not None:\n",
        "                X_val = np.c_[np.ones(X_val.shape[0]), X_val]\n",
        "\n",
        "        self.coef_ = np.zeros(X.shape[1])\n",
        "\n",
        "        for i in range(self.iter):\n",
        "            y_pred = self._linear_hypothesis(X)\n",
        "            error = y_pred - y\n",
        "            self._gradient_descent(X, error)\n",
        "\n",
        "            self.loss[i] = half_MSE(y_pred, y)\n",
        "\n",
        "            if X_val is not None and y_val is not None:\n",
        "                val_pred = self._linear_hypothesis(X_val)\n",
        "                self.val_loss[i] = half_MSE(val_pred, y_val)\n",
        "\n",
        "            if self.verbose and i % 10 == 0:\n",
        "                print(f\"Iteration {i}: Training Loss = {self.loss[i]:.4f}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        if not self.no_bias:\n",
        "            X = np.c_[np.ones(X.shape[0]), X]\n",
        "        return self._linear_hypothesis(X)\n",
        "\n",
        "# ========== Loss Function ==========\n",
        "\n",
        "def half_MSE(y_pred, y_true):\n",
        "    return 0.5 * np.mean((y_pred - y_true) ** 2)\n",
        "\n",
        "def RMSE(y_pred, y_true):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "# ========== Load and Prepare House Prices Data ==========\n",
        "\n",
        "data = pd.read_csv(\"train.csv\")\n",
        "X = data.select_dtypes(include=[np.number]).drop(columns=[\"Id\", \"SalePrice\"]).fillna(0)\n",
        "y = data[\"SalePrice\"].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X.values, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ========== Train Scratch Model ==========\n",
        "\n",
        "scratch_model = ScratchLinearRegression(num_iter=200, lr=1e-7, no_bias=False, verbose=True)\n",
        "scratch_model.fit(X_train, y_train, X_val=X_test, y_val=y_test)\n",
        "y_pred_scratch = scratch_model.predict(X_test)\n",
        "\n",
        "# ========== Train Scikit-learn Model ==========\n",
        "\n",
        "sk_model = LinearRegression()\n",
        "sk_model.fit(X_train, y_train)\n",
        "y_pred_sk = sk_model.predict(X_test)\n",
        "\n",
        "# ========== Evaluation ==========\n",
        "\n",
        "print(\"\\n--- Comparison ---\")\n",
        "print(\"Scratch RMSE:\", RMSE(y_pred_scratch, y_test))\n",
        "print(\"Scikit-learn RMSE:\", RMSE(y_pred_sk, y_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "0670c59a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 927
        },
        "id": "0670c59a",
        "outputId": "33b9ad4a-c7c8-4376-90de-c39222a0a2b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0: Training Loss = 19442791762.8549, Validation Loss = 19827002217.1022\n",
            "Iteration 10: Training Loss = 19442791752.7373, Validation Loss = 19827002208.1760\n",
            "Iteration 20: Training Loss = 19442791742.6197, Validation Loss = 19827002199.2497\n",
            "Iteration 30: Training Loss = 19442791732.5020, Validation Loss = 19827002190.3234\n",
            "Iteration 40: Training Loss = 19442791722.3844, Validation Loss = 19827002181.3971\n",
            "Iteration 50: Training Loss = 19442791712.2668, Validation Loss = 19827002172.4709\n",
            "Iteration 60: Training Loss = 19442791702.1492, Validation Loss = 19827002163.5446\n",
            "Iteration 70: Training Loss = 19442791692.0316, Validation Loss = 19827002154.6183\n",
            "Iteration 80: Training Loss = 19442791681.9140, Validation Loss = 19827002145.6921\n",
            "Iteration 90: Training Loss = 19442791671.7964, Validation Loss = 19827002136.7658\n",
            "Iteration 100: Training Loss = 19442791661.6788, Validation Loss = 19827002127.8395\n",
            "Iteration 110: Training Loss = 19442791651.5612, Validation Loss = 19827002118.9132\n",
            "Iteration 120: Training Loss = 19442791641.4436, Validation Loss = 19827002109.9870\n",
            "Iteration 130: Training Loss = 19442791631.3260, Validation Loss = 19827002101.0607\n",
            "Iteration 140: Training Loss = 19442791621.2083, Validation Loss = 19827002092.1344\n",
            "Iteration 150: Training Loss = 19442791611.0907, Validation Loss = 19827002083.2082\n",
            "Iteration 160: Training Loss = 19442791600.9731, Validation Loss = 19827002074.2819\n",
            "Iteration 170: Training Loss = 19442791590.8555, Validation Loss = 19827002065.3556\n",
            "Iteration 180: Training Loss = 19442791580.7379, Validation Loss = 19827002056.4293\n",
            "Iteration 190: Training Loss = 19442791570.6203, Validation Loss = 19827002047.5031\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Define the MSE function\n",
        "def half_MSE(y_pred, y):\n",
        "    \"\"\"\n",
        "    Mean squared error calculation (scaled by 1/2).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y_pred : ndarray, shape (n_samples,)\n",
        "        Predicted values\n",
        "    y : ndarray, shape (n_samples,)\n",
        "        True values\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    mse : float\n",
        "        Half of the mean squared error\n",
        "    \"\"\"\n",
        "    return np.mean((y_pred - y) ** 2) / 2\n",
        "\n",
        "# ScratchLinearRegression class\n",
        "class ScratchLinearRegression:\n",
        "    def __init__(self, num_iter=100, lr=1e-6, no_bias=False, verbose=False):\n",
        "        self.iter = num_iter\n",
        "        self.lr = lr\n",
        "        self.no_bias = no_bias\n",
        "        self.verbose = verbose\n",
        "        self.loss = np.zeros(self.iter)  # Training loss\n",
        "        self.val_loss = np.zeros(self.iter)  # Validation loss\n",
        "\n",
        "    def _linear_hypothesis(self, X):\n",
        "        return X @ self.coef_\n",
        "\n",
        "    def _gradient_descent(self, X, error, clip_value=1.0):\n",
        "        m = X.shape[0]\n",
        "        gradient = (X.T @ error) / m\n",
        "        # Clip gradients to avoid excessively large updates\n",
        "        np.clip(gradient, -clip_value, clip_value, out=gradient)\n",
        "        self.coef_ -= self.lr * gradient\n",
        "\n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        if not self.no_bias:\n",
        "            X = np.c_[np.ones(X.shape[0]), X]\n",
        "            if X_val is not None:\n",
        "                X_val = np.c_[np.ones(X_val.shape[0]), X_val]\n",
        "\n",
        "        self.coef_ = np.zeros(X.shape[1])\n",
        "\n",
        "        for i in range(self.iter):\n",
        "            # Training phase\n",
        "            y_pred = self._linear_hypothesis(X)\n",
        "            error = y_pred - y\n",
        "            self._gradient_descent(X, error)\n",
        "\n",
        "            # Record training loss\n",
        "            self.loss[i] = half_MSE(y_pred, y)\n",
        "\n",
        "            # Record validation loss if available\n",
        "            if X_val is not None and y_val is not None:\n",
        "                val_pred = self._linear_hypothesis(X_val)\n",
        "                self.val_loss[i] = half_MSE(val_pred, y_val)\n",
        "\n",
        "            # Verbose output every 10 iterations for debugging\n",
        "            if self.verbose and i % 10 == 0:\n",
        "                print(f\"Iteration {i}: Training Loss = {self.loss[i]:.4f}, Validation Loss = {self.val_loss[i]:.4f}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        if not self.no_bias:\n",
        "            X = np.c_[np.ones(X.shape[0]), X]\n",
        "        return self._linear_hypothesis(X)\n",
        "\n",
        "# Function to plot learning curve\n",
        "def plot_learning_curve(train_loss, val_loss):\n",
        "    \"\"\"\n",
        "    Plot the learning curve showing training and validation losses.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    train_loss : ndarray\n",
        "        Array containing the training loss values.\n",
        "    val_loss : ndarray\n",
        "        Array containing the validation loss values.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(train_loss, label='Training Loss')\n",
        "    plt.plot(val_loss, label='Validation Loss', linestyle='--')\n",
        "    plt.xlabel('Iterations')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Learning Curve')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Example: Load and prepare data for the House Prices competition\n",
        "data = pd.read_csv(\"train.csv\")\n",
        "X = data.select_dtypes(include=[np.number]).drop(columns=[\"Id\", \"SalePrice\"]).fillna(0)\n",
        "y = data[\"SalePrice\"].values\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X.values, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Scratch model with normalized features and smaller learning rate\n",
        "scratch_model = ScratchLinearRegression(num_iter=200, lr=1e-6, no_bias=False, verbose=True)\n",
        "scratch_model.fit(X_train_scaled, y_train, X_val=X_test_scaled, y_val=y_test)\n",
        "\n",
        "# Plot the learning curve\n",
        "plot_learning_curve(scratch_model.loss, scratch_model.val_loss)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13b74594",
      "metadata": {
        "id": "13b74594"
      },
      "source": [
        "##### 【Problem 8 】 (Advance Challenge) Removal of Bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "4be6df31",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4be6df31",
        "outputId": "e3f0fa58-56b9-431a-c6a2-86fbd113f71d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0: Training Loss = 19442791762.8549, Validation Loss = 19826401761.2824\n",
            "Iteration 10: Training Loss = 19436500798.5760, Validation Loss = 19820399642.3126\n",
            "Iteration 20: Training Loss = 19430214505.5478, Validation Loss = 19814401970.6906\n",
            "Iteration 30: Training Loss = 19423932878.2188, Validation Loss = 19808408740.9322\n",
            "Iteration 40: Training Loss = 19417655911.0451, Validation Loss = 19802419947.5602\n",
            "Iteration 50: Training Loss = 19411383598.4900, Validation Loss = 19796435585.1054\n",
            "Iteration 60: Training Loss = 19405115935.0243, Validation Loss = 19790455648.1056\n",
            "Iteration 70: Training Loss = 19398852915.1260, Validation Loss = 19784480131.1065\n",
            "Iteration 80: Training Loss = 19392594533.2806, Validation Loss = 19778509028.6608\n",
            "Iteration 90: Training Loss = 19386340783.9806, Validation Loss = 19772542335.3289\n",
            "Iteration 100: Training Loss = 19380091661.7260, Validation Loss = 19766580045.6786\n",
            "Iteration 110: Training Loss = 19373847161.0242, Validation Loss = 19760622154.2851\n",
            "Iteration 120: Training Loss = 19367607276.3896, Validation Loss = 19754668655.7308\n",
            "Iteration 130: Training Loss = 19361372002.3440, Validation Loss = 19748719544.6058\n",
            "Iteration 140: Training Loss = 19355141333.4166, Validation Loss = 19742774815.5074\n",
            "Iteration 150: Training Loss = 19348915264.1437, Validation Loss = 19736834463.0403\n",
            "Iteration 160: Training Loss = 19342693789.0688, Validation Loss = 19730898481.8166\n",
            "Iteration 170: Training Loss = 19336476902.7427, Validation Loss = 19724966866.4556\n",
            "Iteration 180: Training Loss = 19330264599.7234, Validation Loss = 19719039611.5842\n",
            "Iteration 190: Training Loss = 19324056874.5762, Validation Loss = 19713116711.8363\n",
            "Iteration 200: Training Loss = 19317853721.8735, Validation Loss = 19707198161.8535\n",
            "Iteration 210: Training Loss = 19311655136.1949, Validation Loss = 19701283956.2844\n",
            "Iteration 220: Training Loss = 19305461112.1273, Validation Loss = 19695374089.7849\n",
            "Iteration 230: Training Loss = 19299271644.2645, Validation Loss = 19689468557.0185\n",
            "Iteration 240: Training Loss = 19293086727.2077, Validation Loss = 19683567352.6557\n",
            "Iteration 250: Training Loss = 19286906355.5653, Validation Loss = 19677670471.3742\n",
            "Iteration 260: Training Loss = 19280730523.9525, Validation Loss = 19671777907.8593\n",
            "Iteration 270: Training Loss = 19274559226.9921, Validation Loss = 19665889656.8033\n",
            "Iteration 280: Training Loss = 19268392459.3138, Validation Loss = 19660005712.9057\n",
            "Iteration 290: Training Loss = 19262230215.5542, Validation Loss = 19654126070.8733\n",
            "Iteration 300: Training Loss = 19256072490.3573, Validation Loss = 19648250725.4203\n",
            "Iteration 310: Training Loss = 19249919278.3742, Validation Loss = 19642379671.2678\n",
            "Iteration 320: Training Loss = 19243770574.2628, Validation Loss = 19636512903.1443\n",
            "Iteration 330: Training Loss = 19237626372.6885, Validation Loss = 19630650415.7853\n",
            "Iteration 340: Training Loss = 19231486668.3234, Validation Loss = 19624792203.9339\n",
            "Iteration 350: Training Loss = 19225351455.8468, Validation Loss = 19618938262.3398\n",
            "Iteration 360: Training Loss = 19219220729.9450, Validation Loss = 19613088585.7602\n",
            "Iteration 370: Training Loss = 19213094485.3114, Validation Loss = 19607243168.9595\n",
            "Iteration 380: Training Loss = 19206972716.6465, Validation Loss = 19601402006.7091\n",
            "Iteration 390: Training Loss = 19200855418.6576, Validation Loss = 19595565093.7874\n",
            "Iteration 400: Training Loss = 19194742586.0592, Validation Loss = 19589732424.9803\n",
            "Iteration 410: Training Loss = 19188634213.5727, Validation Loss = 19583903995.0804\n",
            "Iteration 420: Training Loss = 19182530295.9266, Validation Loss = 19578079798.8877\n",
            "Iteration 430: Training Loss = 19176430827.8562, Validation Loss = 19572259831.2091\n",
            "Iteration 440: Training Loss = 19170335804.1039, Validation Loss = 19566444086.8588\n",
            "Iteration 450: Training Loss = 19164245219.4191, Validation Loss = 19560632560.6578\n",
            "Iteration 460: Training Loss = 19158159068.5580, Validation Loss = 19554825247.4343\n",
            "Iteration 470: Training Loss = 19152077346.2839, Validation Loss = 19549022142.0237\n",
            "Iteration 480: Training Loss = 19146000047.3670, Validation Loss = 19543223239.2681\n",
            "Iteration 490: Training Loss = 19139927166.5843, Validation Loss = 19537428534.0168\n",
            "Iteration 0: Training Loss = 19442791762.8549, Validation Loss = 19826726250.2297\n",
            "Iteration 10: Training Loss = 19439792589.1596, Validation Loss = 19823968758.1515\n",
            "Iteration 20: Training Loss = 19436797428.4193, Validation Loss = 19821215235.9446\n",
            "Iteration 30: Training Loss = 19433806275.2143, Validation Loss = 19818465678.0470\n",
            "Iteration 40: Training Loss = 19430819124.1323, Validation Loss = 19815720078.9042\n",
            "Iteration 50: Training Loss = 19427835969.7683, Validation Loss = 19812978432.9693\n",
            "Iteration 60: Training Loss = 19424856806.7245, Validation Loss = 19810240734.7032\n",
            "Iteration 70: Training Loss = 19421881629.6105, Validation Loss = 19807506978.5742\n",
            "Iteration 80: Training Loss = 19418910433.0431, Validation Loss = 19804777159.0586\n",
            "Iteration 90: Training Loss = 19415943211.6467, Validation Loss = 19802051270.6399\n",
            "Iteration 100: Training Loss = 19412979960.0526, Validation Loss = 19799329307.8095\n",
            "Iteration 110: Training Loss = 19410020672.8994, Validation Loss = 19796611265.0661\n",
            "Iteration 120: Training Loss = 19407065344.8333, Validation Loss = 19793897136.9163\n",
            "Iteration 130: Training Loss = 19404113970.5072, Validation Loss = 19791186917.8739\n",
            "Iteration 140: Training Loss = 19401166544.5818, Validation Loss = 19788480602.4607\n",
            "Iteration 150: Training Loss = 19398223061.7246, Validation Loss = 19785778185.2057\n",
            "Iteration 160: Training Loss = 19395283516.6105, Validation Loss = 19783079660.6456\n",
            "Iteration 170: Training Loss = 19392347903.9216, Validation Loss = 19780385023.3245\n",
            "Iteration 180: Training Loss = 19389416218.3471, Validation Loss = 19777694267.7940\n",
            "Iteration 190: Training Loss = 19386488454.5836, Validation Loss = 19775007388.6135\n",
            "Iteration 200: Training Loss = 19383564607.3345, Validation Loss = 19772324380.3497\n",
            "Iteration 210: Training Loss = 19380644671.3108, Validation Loss = 19769645237.5765\n",
            "Iteration 220: Training Loss = 19377728641.2304, Validation Loss = 19766969954.8758\n",
            "Iteration 230: Training Loss = 19374816511.8183, Validation Loss = 19764298526.8367\n",
            "Iteration 240: Training Loss = 19371908277.8069, Validation Loss = 19761630948.0556\n",
            "Iteration 250: Training Loss = 19369003933.9353, Validation Loss = 19758967213.1367\n",
            "Iteration 260: Training Loss = 19366103474.9503, Validation Loss = 19756307316.6913\n",
            "Iteration 270: Training Loss = 19363206895.6052, Validation Loss = 19753651253.3384\n",
            "Iteration 280: Training Loss = 19360314190.6608, Validation Loss = 19750999017.7042\n",
            "Iteration 290: Training Loss = 19357425354.8849, Validation Loss = 19748350604.4224\n",
            "Iteration 300: Training Loss = 19354540383.0523, Validation Loss = 19745706008.1341\n",
            "Iteration 310: Training Loss = 19351659269.9450, Validation Loss = 19743065223.4878\n",
            "Iteration 320: Training Loss = 19348782010.3518, Validation Loss = 19740428245.1392\n",
            "Iteration 330: Training Loss = 19345908599.0689, Validation Loss = 19737795067.7517\n",
            "Iteration 340: Training Loss = 19343039030.8993, Validation Loss = 19735165685.9957\n",
            "Iteration 350: Training Loss = 19340173300.6531, Validation Loss = 19732540094.5492\n",
            "Iteration 360: Training Loss = 19337311403.1474, Validation Loss = 19729918288.0974\n",
            "Iteration 370: Training Loss = 19334453333.2063, Validation Loss = 19727300261.3329\n",
            "Iteration 380: Training Loss = 19331599085.6610, Validation Loss = 19724686008.9554\n",
            "Iteration 390: Training Loss = 19328748655.3496, Validation Loss = 19722075525.6723\n",
            "Iteration 400: Training Loss = 19325902037.1173, Validation Loss = 19719468806.1979\n",
            "Iteration 410: Training Loss = 19323059225.8160, Validation Loss = 19716865845.2540\n",
            "Iteration 420: Training Loss = 19320220216.3049, Validation Loss = 19714266637.5696\n",
            "Iteration 430: Training Loss = 19317385003.4499, Validation Loss = 19711671177.8811\n",
            "Iteration 440: Training Loss = 19314553582.1240, Validation Loss = 19709079460.9319\n",
            "Iteration 450: Training Loss = 19311725947.2071, Validation Loss = 19706491481.4728\n",
            "Iteration 460: Training Loss = 19308902093.5860, Validation Loss = 19703907234.2619\n",
            "Iteration 470: Training Loss = 19306082016.1544, Validation Loss = 19701326714.0644\n",
            "Iteration 480: Training Loss = 19303265709.8129, Validation Loss = 19698749915.6527\n",
            "Iteration 490: Training Loss = 19300453169.4692, Validation Loss = 19696176833.8066\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Define MSE function (unchanged)\n",
        "def half_MSE(y_pred, y):\n",
        "    \"\"\"\n",
        "    Mean squared error calculation (scaled by 1/2).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y_pred : ndarray, shape (n_samples,)\n",
        "        Predicted values\n",
        "    y : ndarray, shape (n_samples,)\n",
        "        True values\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    mse : float\n",
        "        Half of the mean squared error\n",
        "    \"\"\"\n",
        "    return np.mean((y_pred - y) ** 2) / 2\n",
        "\n",
        "# ScratchLinearRegression Class with an option to remove bias term\n",
        "class ScratchLinearRegression:\n",
        "    def __init__(self, num_iter=100, lr=1e-6, no_bias=False, verbose=False):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "        ----------\n",
        "        num_iter : int\n",
        "            Number of iterations for gradient descent.\n",
        "        lr : float\n",
        "            Learning rate.\n",
        "        no_bias : bool\n",
        "            Whether to exclude the bias term.\n",
        "        verbose : bool\n",
        "            Whether to print the progress during training.\n",
        "        \"\"\"\n",
        "        self.iter = num_iter\n",
        "        self.lr = lr\n",
        "        self.no_bias = no_bias\n",
        "        self.verbose = verbose\n",
        "        self.loss = np.zeros(self.iter)  # To track training loss\n",
        "        self.val_loss = np.zeros(self.iter)  # To track validation loss\n",
        "\n",
        "    def _linear_hypothesis(self, X):\n",
        "        \"\"\"\n",
        "        Calculate the hypothesis (h_theta(x)).\n",
        "\n",
        "        Parameters:\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            Input data (features).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        ndarray, shape (n_samples,)\n",
        "            Predicted values (h_theta(x)).\n",
        "        \"\"\"\n",
        "        return X @ self.coef_\n",
        "\n",
        "    def _gradient_descent(self, X, error):\n",
        "        \"\"\"\n",
        "        Perform gradient descent to update the model parameters.\n",
        "\n",
        "        Parameters:\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            Input data (features).\n",
        "        error : ndarray, shape (n_samples,)\n",
        "            Difference between predicted and actual values.\n",
        "        \"\"\"\n",
        "        m = X.shape[0]\n",
        "        gradient = (X.T @ error) / m\n",
        "        self.coef_ -= self.lr * gradient\n",
        "\n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        \"\"\"\n",
        "        Train the linear regression model using gradient descent.\n",
        "\n",
        "        Parameters:\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            Training data (features).\n",
        "        y : ndarray, shape (n_samples,)\n",
        "            Training labels (true values).\n",
        "        X_val : ndarray, shape (n_samples, n_features), optional\n",
        "            Validation data (features).\n",
        "        y_val : ndarray, shape (n_samples,), optional\n",
        "            Validation labels (true values).\n",
        "        \"\"\"\n",
        "        if not self.no_bias:\n",
        "            X = np.c_[np.ones(X.shape[0]), X]  # Add bias term (1) to X\n",
        "            if X_val is not None:\n",
        "                X_val = np.c_[np.ones(X_val.shape[0]), X_val]  # Add bias term to X_val\n",
        "\n",
        "        # Initialize coefficients to zeros (including the bias term if no_bias is False)\n",
        "        self.coef_ = np.zeros(X.shape[1])\n",
        "\n",
        "        for i in range(self.iter):\n",
        "            # Training phase\n",
        "            y_pred = self._linear_hypothesis(X)\n",
        "            error = y_pred - y\n",
        "            self._gradient_descent(X, error)\n",
        "\n",
        "            # Record training loss\n",
        "            self.loss[i] = half_MSE(y_pred, y)\n",
        "\n",
        "            # Record validation loss if available\n",
        "            if X_val is not None and y_val is not None:\n",
        "                val_pred = self._linear_hypothesis(X_val)\n",
        "                self.val_loss[i] = half_MSE(val_pred, y_val)\n",
        "\n",
        "            # Verbose output every 10 iterations for debugging\n",
        "            if self.verbose and i % 10 == 0:\n",
        "                print(f\"Iteration {i}: Training Loss = {self.loss[i]:.4f}, Validation Loss = {self.val_loss[i]:.4f}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict using the trained linear regression model.\n",
        "\n",
        "        Parameters:\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            Input data (features).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        ndarray, shape (n_samples,)\n",
        "            Predicted values.\n",
        "        \"\"\"\n",
        "        if not self.no_bias:\n",
        "            X = np.c_[np.ones(X.shape[0]), X]  # Add bias term if required\n",
        "        return self._linear_hypothesis(X)\n",
        "\n",
        "\n",
        "# Example Usage\n",
        "# Load data (House Prices competition example)\n",
        "data = pd.read_csv(\"train.csv\")\n",
        "X = data.select_dtypes(include=[np.number]).drop(columns=[\"Id\", \"SalePrice\"]).fillna(0)\n",
        "y = data[\"SalePrice\"].values\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X.values, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train models: One with bias and one without bias\n",
        "model_with_bias = ScratchLinearRegression(num_iter=500, lr=1e-5, no_bias=False, verbose=True)\n",
        "model_without_bias = ScratchLinearRegression(num_iter=500, lr=1e-5, no_bias=True, verbose=True)\n",
        "\n",
        "model_with_bias.fit(X_train_scaled, y_train, X_val=X_test_scaled, y_val=y_test)\n",
        "model_without_bias.fit(X_train_scaled, y_train, X_val=X_test_scaled, y_val=y_test)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bb46d90",
      "metadata": {
        "id": "5bb46d90"
      },
      "source": [
        "##### 【Problem 9 】 (Advance Challenge) Multidimensional characterization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "55e48385",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55e48385",
        "outputId": "9711a72d-33c5-4871-b599-785a88f7e055"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0: Training Loss = 19442791762.8549, Validation Loss = 19826401761.2824\n",
            "Iteration 10: Training Loss = 19436500798.5760, Validation Loss = 19820399642.3126\n",
            "Iteration 20: Training Loss = 19430214505.5478, Validation Loss = 19814401970.6906\n",
            "Iteration 30: Training Loss = 19423932878.2188, Validation Loss = 19808408740.9322\n",
            "Iteration 40: Training Loss = 19417655911.0451, Validation Loss = 19802419947.5602\n",
            "Iteration 50: Training Loss = 19411383598.4900, Validation Loss = 19796435585.1054\n",
            "Iteration 60: Training Loss = 19405115935.0243, Validation Loss = 19790455648.1056\n",
            "Iteration 70: Training Loss = 19398852915.1260, Validation Loss = 19784480131.1065\n",
            "Iteration 80: Training Loss = 19392594533.2806, Validation Loss = 19778509028.6608\n",
            "Iteration 90: Training Loss = 19386340783.9806, Validation Loss = 19772542335.3289\n",
            "Iteration 100: Training Loss = 19380091661.7260, Validation Loss = 19766580045.6786\n",
            "Iteration 110: Training Loss = 19373847161.0242, Validation Loss = 19760622154.2851\n",
            "Iteration 120: Training Loss = 19367607276.3896, Validation Loss = 19754668655.7308\n",
            "Iteration 130: Training Loss = 19361372002.3440, Validation Loss = 19748719544.6058\n",
            "Iteration 140: Training Loss = 19355141333.4166, Validation Loss = 19742774815.5074\n",
            "Iteration 150: Training Loss = 19348915264.1437, Validation Loss = 19736834463.0403\n",
            "Iteration 160: Training Loss = 19342693789.0688, Validation Loss = 19730898481.8166\n",
            "Iteration 170: Training Loss = 19336476902.7427, Validation Loss = 19724966866.4556\n",
            "Iteration 180: Training Loss = 19330264599.7234, Validation Loss = 19719039611.5842\n",
            "Iteration 190: Training Loss = 19324056874.5762, Validation Loss = 19713116711.8363\n",
            "Iteration 200: Training Loss = 19317853721.8735, Validation Loss = 19707198161.8535\n",
            "Iteration 210: Training Loss = 19311655136.1949, Validation Loss = 19701283956.2844\n",
            "Iteration 220: Training Loss = 19305461112.1273, Validation Loss = 19695374089.7849\n",
            "Iteration 230: Training Loss = 19299271644.2645, Validation Loss = 19689468557.0185\n",
            "Iteration 240: Training Loss = 19293086727.2077, Validation Loss = 19683567352.6557\n",
            "Iteration 250: Training Loss = 19286906355.5653, Validation Loss = 19677670471.3742\n",
            "Iteration 260: Training Loss = 19280730523.9525, Validation Loss = 19671777907.8593\n",
            "Iteration 270: Training Loss = 19274559226.9921, Validation Loss = 19665889656.8033\n",
            "Iteration 280: Training Loss = 19268392459.3138, Validation Loss = 19660005712.9057\n",
            "Iteration 290: Training Loss = 19262230215.5542, Validation Loss = 19654126070.8733\n",
            "Iteration 300: Training Loss = 19256072490.3573, Validation Loss = 19648250725.4203\n",
            "Iteration 310: Training Loss = 19249919278.3742, Validation Loss = 19642379671.2678\n",
            "Iteration 320: Training Loss = 19243770574.2628, Validation Loss = 19636512903.1443\n",
            "Iteration 330: Training Loss = 19237626372.6885, Validation Loss = 19630650415.7853\n",
            "Iteration 340: Training Loss = 19231486668.3234, Validation Loss = 19624792203.9339\n",
            "Iteration 350: Training Loss = 19225351455.8468, Validation Loss = 19618938262.3398\n",
            "Iteration 360: Training Loss = 19219220729.9450, Validation Loss = 19613088585.7602\n",
            "Iteration 370: Training Loss = 19213094485.3114, Validation Loss = 19607243168.9595\n",
            "Iteration 380: Training Loss = 19206972716.6465, Validation Loss = 19601402006.7091\n",
            "Iteration 390: Training Loss = 19200855418.6576, Validation Loss = 19595565093.7874\n",
            "Iteration 400: Training Loss = 19194742586.0592, Validation Loss = 19589732424.9803\n",
            "Iteration 410: Training Loss = 19188634213.5727, Validation Loss = 19583903995.0804\n",
            "Iteration 420: Training Loss = 19182530295.9266, Validation Loss = 19578079798.8877\n",
            "Iteration 430: Training Loss = 19176430827.8562, Validation Loss = 19572259831.2091\n",
            "Iteration 440: Training Loss = 19170335804.1039, Validation Loss = 19566444086.8588\n",
            "Iteration 450: Training Loss = 19164245219.4191, Validation Loss = 19560632560.6578\n",
            "Iteration 460: Training Loss = 19158159068.5580, Validation Loss = 19554825247.4343\n",
            "Iteration 470: Training Loss = 19152077346.2839, Validation Loss = 19549022142.0237\n",
            "Iteration 480: Training Loss = 19146000047.3670, Validation Loss = 19543223239.2681\n",
            "Iteration 490: Training Loss = 19139927166.5843, Validation Loss = 19537428534.0168\n",
            "Iteration 0: Training Loss = 19442791762.8549, Validation Loss = 19813168858.2151\n",
            "Iteration 10: Training Loss = 19291397048.8465, Validation Loss = 19675843357.0500\n",
            "Iteration 20: Training Loss = 19143082787.8930, Validation Loss = 19540322344.1126\n",
            "Iteration 30: Training Loss = 18997731875.4063, Validation Loss = 19406566276.3868\n",
            "Iteration 40: Training Loss = 18855233963.1029, Validation Loss = 19274536919.3293\n",
            "Iteration 50: Training Loss = 18715484941.4091, Validation Loss = 19144197293.2858\n",
            "Iteration 60: Training Loss = 18578386469.1765, Validation Loss = 19015511622.3065\n",
            "Iteration 70: Training Loss = 18443845545.9679, Validation Loss = 18888445285.2515\n",
            "Iteration 80: Training Loss = 18311774122.6660, Validation Loss = 18762964769.0758\n",
            "Iteration 90: Training Loss = 18182088746.6028, Validation Loss = 18639037624.1955\n",
            "Iteration 100: Training Loss = 18054710237.8021, Validation Loss = 18516632421.8351\n",
            "Iteration 110: Training Loss = 17929563393.2814, Validation Loss = 18395718713.2657\n",
            "Iteration 120: Training Loss = 17806576716.6751, Validation Loss = 18276266990.8465\n",
            "Iteration 130: Training Loss = 17685682170.7249, Validation Loss = 18158248650.7840\n",
            "Iteration 140: Training Loss = 17566814950.4351, Validation Loss = 18041635957.5313\n",
            "Iteration 150: Training Loss = 17449913274.9160, Validation Loss = 17926402009.7517\n",
            "Iteration 160: Training Loss = 17334918196.1431, Validation Loss = 17812520707.7722\n",
            "Iteration 170: Training Loss = 17221773423.0369, Validation Loss = 17699966722.4616\n",
            "Iteration 180: Training Loss = 17110425159.4346, Validation Loss = 17588715465.4647\n",
            "Iteration 190: Training Loss = 17000821954.6652, Validation Loss = 17478743060.7325\n",
            "Iteration 200: Training Loss = 16892914565.5724, Validation Loss = 17370026317.2869\n",
            "Iteration 210: Training Loss = 16786655828.9449, Validation Loss = 17262542703.1663\n",
            "Iteration 220: Training Loss = 16682000543.4159, Validation Loss = 17156270320.4944\n",
            "Iteration 230: Training Loss = 16578905359.9899, Validation Loss = 17051187881.6246\n",
            "Iteration 240: Training Loss = 16477328680.4349, Validation Loss = 16947274686.3083\n",
            "Iteration 250: Training Loss = 16377230562.8563, Validation Loss = 16844510599.8422\n",
            "Iteration 260: Training Loss = 16278572633.8314, Validation Loss = 16742876032.1493\n",
            "Iteration 270: Training Loss = 16181318006.5485, Validation Loss = 16642351917.7508\n",
            "Iteration 280: Training Loss = 16085431204.4448, Validation Loss = 16542919696.5910\n",
            "Iteration 290: Training Loss = 15990878089.8862, Validation Loss = 16444561295.6724\n",
            "Iteration 300: Training Loss = 15897625797.4781, Validation Loss = 16347259111.4693\n",
            "Iteration 310: Training Loss = 15805642671.6307, Validation Loss = 16250995993.0808\n",
            "Iteration 320: Training Loss = 15714898208.0424, Validation Loss = 16155755226.0923\n",
            "Iteration 330: Training Loss = 15625362998.7921, Validation Loss = 16061520517.1129\n",
            "Iteration 340: Training Loss = 15537008680.7624, Validation Loss = 15968275978.9580\n",
            "Iteration 350: Training Loss = 15449807887.1389, Validation Loss = 15876006116.4491\n",
            "Iteration 360: Training Loss = 15363734201.7560, Validation Loss = 15784695812.8026\n",
            "Iteration 370: Training Loss = 15278762116.0781, Validation Loss = 15694330316.5814\n",
            "Iteration 380: Training Loss = 15194866988.6254, Validation Loss = 15604895229.1847\n",
            "Iteration 390: Training Loss = 15112025006.6688, Validation Loss = 15516376492.8505\n",
            "Iteration 400: Training Loss = 15030213150.0346, Validation Loss = 15428760379.1509\n",
            "Iteration 410: Training Loss = 14949409156.8727, Validation Loss = 15342033477.9552\n",
            "Iteration 420: Training Loss = 14869591491.2553, Validation Loss = 15256182686.8421\n",
            "Iteration 430: Training Loss = 14790739312.4823, Validation Loss = 15171195200.9411\n",
            "Iteration 440: Training Loss = 14712832445.9821, Validation Loss = 15087058503.1825\n",
            "Iteration 450: Training Loss = 14635851355.7039, Validation Loss = 15003760354.9404\n",
            "Iteration 460: Training Loss = 14559777117.9060, Validation Loss = 14921288787.0494\n",
            "Iteration 470: Training Loss = 14484591396.2540, Validation Loss = 14839632091.1799\n",
            "Iteration 480: Training Loss = 14410276418.1461, Validation Loss = 14758778811.5560\n",
            "Iteration 490: Training Loss = 14336814952.1925, Validation Loss = 14678717736.9999\n",
            "Mean Squared Error (Original Features): 19532216883.9056\n",
            "Mean Squared Error (Squared Features): 14607331030.2622\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# ScratchLinearRegression Class and other necessary code as defined previously...\n",
        "\n",
        "def add_squared_features(X):\n",
        "    \"\"\"\n",
        "    Add squared features to the original feature set.\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    X : ndarray, shape (n_samples, n_features)\n",
        "        Input data (features).\n",
        "\n",
        "    Returns:\n",
        "    -------\n",
        "    ndarray, shape (n_samples, 2 * n_features)\n",
        "        Extended feature set including squared features.\n",
        "    \"\"\"\n",
        "    X_squared = np.square(X)  # Square each feature\n",
        "    return np.c_[X, X_squared]  # Combine original and squared features\n",
        "\n",
        "# Example Usage with Squared Features:\n",
        "# Load data (House Prices competition example)\n",
        "data = pd.read_csv(\"train.csv\")\n",
        "X = data.select_dtypes(include=[np.number]).drop(columns=[\"Id\", \"SalePrice\"]).fillna(0)\n",
        "y = data[\"SalePrice\"].values\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X.values, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 1. Train Model with Original Features\n",
        "model_original = ScratchLinearRegression(num_iter=500, lr=1e-5, no_bias=False, verbose=True)\n",
        "model_original.fit(X_train_scaled, y_train, X_val=X_test_scaled, y_val=y_test)\n",
        "\n",
        "# 2. Train Model with Squared Features\n",
        "X_train_squared = add_squared_features(X_train_scaled)\n",
        "X_test_squared = add_squared_features(X_test_scaled)\n",
        "model_squared = ScratchLinearRegression(num_iter=500, lr=1e-5, no_bias=False, verbose=True)\n",
        "model_squared.fit(X_train_squared, y_train, X_val=X_test_squared, y_val=y_test)\n",
        "\n",
        "\n",
        "# You can also evaluate the performance by calculating the final MSE for both models\n",
        "y_pred_original = model_original.predict(X_test_scaled)\n",
        "y_pred_squared = model_squared.predict(X_test_squared)\n",
        "\n",
        "mse_original = half_MSE(y_pred_original, y_test)\n",
        "mse_squared = half_MSE(y_pred_squared, y_test)\n",
        "\n",
        "print(f\"Mean Squared Error (Original Features): {mse_original:.4f}\")\n",
        "print(f\"Mean Squared Error (Squared Features): {mse_squared:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "256ad311",
      "metadata": {
        "id": "256ad311"
      },
      "source": [
        "##### 【Problem 10 】 (Advance Challenge) Update derivation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4fd2051",
      "metadata": {
        "id": "a4fd2051"
      },
      "source": [
        "###### Problem 10: Derivation of Gradient Descent Update Rule\n",
        "\n",
        "##### To update parameters \\(\\theta_j\\) in linear regression using the **gradient descent method**, we start from the **loss function** and derive the update rule.\n",
        "\n",
        "##### Loss Function (Mean Squared Error)\n",
        "\n",
        "\\[\n",
        "J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right)^2\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\(m\\): Number of training samples  \n",
        "- \\(h_\\theta(x^{(i)}) = \\theta^T x^{(i)}\\): Hypothesis/prediction  \n",
        "- \\(y^{(i)}\\): Ground truth for the \\(i\\)-th sample  \n",
        "\n",
        "##### Gradient of the Loss w.r.t. \\(\\theta_j\\)\n",
        "\n",
        "\\[\n",
        "\\frac{\\partial}{\\partial \\theta_j} J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right) x^{(i)}_j\n",
        "\\]\n",
        "\n",
        "This gives the slope (gradient) of the loss function with respect to each weight.\n",
        "\n",
        "##### Gradient Descent Update Rule\n",
        "\n",
        "\\[\n",
        "\\theta_j := \\theta_j - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right) x^{(i)}_j\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\(\\alpha\\): Learning rate  \n",
        "- The summation is the average gradient across all samples\n",
        "\n",
        "This formula tells us to subtract a scaled gradient from each \\(\\theta_j\\), moving downhill on the loss function surface.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7dd26534",
      "metadata": {
        "id": "7dd26534"
      },
      "source": [
        "##### 【Problem 10 】 (Advance Challenge) Update derivation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e49ca7f",
      "metadata": {
        "id": "7e49ca7f"
      },
      "source": [
        "#####  Problem 11: Why Linear Regression Doesn’t Get Stuck in Local Minima\n",
        "\n",
        "Unlike more complex models, **linear regression is guaranteed to find a global optimum** when using gradient descent. Here's why:\n",
        "\n",
        "#####  The Cost Function is Convex\n",
        "\n",
        "The loss function:\n",
        "\n",
        "\\[\n",
        "J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right)^2\n",
        "\\]\n",
        "\n",
        "is a **convex quadratic function** with respect to \\(\\theta\\).\n",
        "\n",
        "#####  Properties of Convex Functions:\n",
        "- They have **only one minimum** (no local minima).\n",
        "- Any local minimum is also the **global minimum**.\n",
        "- The surface of the cost function is a **bowl-shaped paraboloid**.\n",
        "\n",
        "#####  Implication for Gradient Descent\n",
        "\n",
        "Gradient descent, when applied to convex functions:\n",
        "- Will **always converge** to the global minimum, if:\n",
        "  - The learning rate is appropriate\n",
        "  - There is enough iteration\n",
        "- This ensures **stable, repeatable results** for linear regression\n",
        "\n",
        "#####  Optional Visualization Code (Python)\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def cost_function(theta):\n",
        "    return (theta - 3)**2 + 5  # Example quadratic\n",
        "\n",
        "theta_vals = np.linspace(-5, 10, 100)\n",
        "cost_vals = cost_function(theta_vals)\n",
        "\n",
        "plt.plot(theta_vals, cost_vals)\n",
        "plt.title('Convex Loss Function in Linear Regression')\n",
        "plt.xlabel('Theta')\n",
        "plt.ylabel('Cost J(θ)')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}